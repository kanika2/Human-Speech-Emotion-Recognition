{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist= os.listdir('/home/kanika/Documents/Major/Speech-Emotion-Analyzer-master/Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03-01-07-01-02-02-20.wav\n",
      "1440\n"
     ]
    }
   ],
   "source": [
    "print(mylist[1400])\n",
    "print(len(mylist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02\n"
     ]
    }
   ],
   "source": [
    "print(mylist[400][6:-16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the audio file's waveform and its spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, sampling_rate = librosa.load('RawData/f11 (2).wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-116ace8fe9e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaveplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob \n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'RawData/f10 (2).wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-148b62b4fd4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwavfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RawData/f10 (2).wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m## Parameters: 10ms step, 30ms window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/minor/lib/python3.7/site-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(filename, mmap)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mmmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'RawData/f10 (2).wav'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "sr,x = scipy.io.wavfile.read('RawData/f10 (2).wav')\n",
    "\n",
    "## Parameters: 10ms step, 30ms window\n",
    "nstep = int(sr * 0.01)\n",
    "nwin  = int(sr * 0.03)\n",
    "nfft = nwin\n",
    "\n",
    "window = np.hamming(nwin)\n",
    "\n",
    "## will take windows x[n1:n2].  generate\n",
    "## and loop over n2 such that all frames\n",
    "## fit within the waveform\n",
    "nn = range(nwin, len(x), nstep)\n",
    "\n",
    "X = np.zeros( (len(nn), nfft//2) )\n",
    "\n",
    "for i,n in enumerate(nn):\n",
    "    xseg = x[n-nwin:n]\n",
    "    z = np.fft.fft(window * xseg, nfft)\n",
    "    X[i,:] = np.log(np.abs(z[:nfft//2]))\n",
    "\n",
    "plt.imshow(X.T, interpolation='nearest',\n",
    "    origin='lower',\n",
    "    aspect='auto')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeling_list=[]\n",
    "for item in mylist:\n",
    "    if item[6:-16]=='02' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_calm')\n",
    "    elif item[6:-16]=='02' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_calm')\n",
    "    elif item[6:-16]=='03' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_happy')\n",
    "    elif item[6:-16]=='03' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_happy')\n",
    "    elif item[6:-16]=='04' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_sad')\n",
    "    elif item[6:-16]=='04' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_sad')\n",
    "    elif item[6:-16]=='05' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_angry')\n",
    "    elif item[6:-16]=='05' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_angry')\n",
    "    elif item[6:-16]=='06' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_fearful')\n",
    "    elif item[6:-16]=='06' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_fearful')\n",
    "    elif item[:1]=='a':\n",
    "        feeling_list.append('male_angry')\n",
    "    elif item[:1]=='f':\n",
    "        feeling_list.append('male_fearful')\n",
    "    elif item[:1]=='h':\n",
    "        feeling_list.append('male_happy')\n",
    "    #elif item[:1]=='n':\n",
    "        #feeling_list.append('neutral')\n",
    "    elif item[:2]=='sa':\n",
    "        feeling_list.append('male_sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(feeling_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "0    male_fearful\n",
       "1     female_calm\n",
       "2      female_sad\n",
       "3      female_sad\n",
       "4  female_fearful\n",
       "5    female_happy\n",
       "6       male_calm\n",
       "7    male_fearful\n",
       "8      male_angry\n",
       "9     female_calm"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the features of audio files using librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for index,y in enumerate(mylist):\n",
    "    if mylist[index][6:-16]!='01' and mylist[index][6:-16]!='07' and mylist[index][6:-16]!='08' and mylist[index][:2]!='su' and mylist[index][:1]!='n' and mylist[index][:1]!='d':\n",
    "        X, sample_rate = librosa.load('/home/kanika/Documents/Major/Speech-Emotion-Analyzer-master/Data/'+y, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "        sample_rate = np.array(sample_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        #[float(i) for i in feature]\n",
    "        #feature1=feature[:135]\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark=bookmark+1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-57.32151, -57.83526, -61.016624, -62.746513,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-68.50275, -68.50275, -68.50275, -68.50275, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-66.8854, -66.8854, -66.8854, -66.8854, -66.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-54.660435, -54.80113, -54.17485, -56.207012,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-55.21731, -57.142357, -56.97951, -56.66498, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature\n",
       "0  [-57.32151, -57.83526, -61.016624, -62.746513,...\n",
       "1  [-68.50275, -68.50275, -68.50275, -68.50275, -...\n",
       "2  [-66.8854, -66.8854, -66.8854, -66.8854, -66.8...\n",
       "3  [-54.660435, -54.80113, -54.17485, -56.207012,...\n",
       "4  [-55.21731, -57.142357, -56.97951, -56.66498, ..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(df['feature'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.concat([df3,labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnewdf = newdf.rename(index=str, columns={\"0\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-57.321510</td>\n",
       "      <td>-57.835258</td>\n",
       "      <td>-61.016624</td>\n",
       "      <td>-62.746513</td>\n",
       "      <td>-58.665215</td>\n",
       "      <td>-60.775200</td>\n",
       "      <td>-61.271671</td>\n",
       "      <td>-56.750999</td>\n",
       "      <td>-56.353233</td>\n",
       "      <td>-56.273521</td>\n",
       "      <td>...</td>\n",
       "      <td>-50.658981</td>\n",
       "      <td>-50.314735</td>\n",
       "      <td>-51.468235</td>\n",
       "      <td>-53.492142</td>\n",
       "      <td>-53.449692</td>\n",
       "      <td>-55.136826</td>\n",
       "      <td>-56.441799</td>\n",
       "      <td>-53.876381</td>\n",
       "      <td>-53.726170</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-68.502747</td>\n",
       "      <td>-68.502747</td>\n",
       "      <td>-68.502747</td>\n",
       "      <td>-68.502747</td>\n",
       "      <td>-68.502747</td>\n",
       "      <td>-68.502747</td>\n",
       "      <td>-68.502747</td>\n",
       "      <td>-68.502747</td>\n",
       "      <td>-68.502747</td>\n",
       "      <td>-68.502747</td>\n",
       "      <td>...</td>\n",
       "      <td>-59.474018</td>\n",
       "      <td>-63.432533</td>\n",
       "      <td>-64.038330</td>\n",
       "      <td>-58.799248</td>\n",
       "      <td>-57.890312</td>\n",
       "      <td>-62.155491</td>\n",
       "      <td>-68.502747</td>\n",
       "      <td>-68.336723</td>\n",
       "      <td>-68.385658</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-66.885399</td>\n",
       "      <td>-66.885399</td>\n",
       "      <td>-66.885399</td>\n",
       "      <td>-66.885399</td>\n",
       "      <td>-66.885399</td>\n",
       "      <td>-66.885399</td>\n",
       "      <td>-66.885399</td>\n",
       "      <td>-66.885399</td>\n",
       "      <td>-66.885399</td>\n",
       "      <td>-66.885399</td>\n",
       "      <td>...</td>\n",
       "      <td>-64.676285</td>\n",
       "      <td>-59.603863</td>\n",
       "      <td>-58.689316</td>\n",
       "      <td>-61.263062</td>\n",
       "      <td>-62.019516</td>\n",
       "      <td>-65.621262</td>\n",
       "      <td>-63.282192</td>\n",
       "      <td>-57.725925</td>\n",
       "      <td>-56.445858</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-54.660435</td>\n",
       "      <td>-54.801128</td>\n",
       "      <td>-54.174850</td>\n",
       "      <td>-56.207012</td>\n",
       "      <td>-52.512180</td>\n",
       "      <td>-53.809551</td>\n",
       "      <td>-54.603172</td>\n",
       "      <td>-55.583664</td>\n",
       "      <td>-54.093208</td>\n",
       "      <td>-53.048031</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.305706</td>\n",
       "      <td>-54.098843</td>\n",
       "      <td>-55.128151</td>\n",
       "      <td>-60.419147</td>\n",
       "      <td>-55.361618</td>\n",
       "      <td>-53.379673</td>\n",
       "      <td>-55.778790</td>\n",
       "      <td>-61.106613</td>\n",
       "      <td>-61.522129</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-55.217312</td>\n",
       "      <td>-57.142357</td>\n",
       "      <td>-56.979511</td>\n",
       "      <td>-56.664982</td>\n",
       "      <td>-57.439548</td>\n",
       "      <td>-57.614170</td>\n",
       "      <td>-57.124355</td>\n",
       "      <td>-56.951164</td>\n",
       "      <td>-56.631126</td>\n",
       "      <td>-56.283089</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.750137</td>\n",
       "      <td>-56.681755</td>\n",
       "      <td>-56.086063</td>\n",
       "      <td>-57.121132</td>\n",
       "      <td>-57.198044</td>\n",
       "      <td>-57.852798</td>\n",
       "      <td>-57.664429</td>\n",
       "      <td>-55.603500</td>\n",
       "      <td>-54.206291</td>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5    \\\n",
       "0 -57.321510 -57.835258 -61.016624 -62.746513 -58.665215 -60.775200   \n",
       "1 -68.502747 -68.502747 -68.502747 -68.502747 -68.502747 -68.502747   \n",
       "2 -66.885399 -66.885399 -66.885399 -66.885399 -66.885399 -66.885399   \n",
       "3 -54.660435 -54.801128 -54.174850 -56.207012 -52.512180 -53.809551   \n",
       "4 -55.217312 -57.142357 -56.979511 -56.664982 -57.439548 -57.614170   \n",
       "\n",
       "         6          7          8          9    ...        207        208  \\\n",
       "0 -61.271671 -56.750999 -56.353233 -56.273521  ... -50.658981 -50.314735   \n",
       "1 -68.502747 -68.502747 -68.502747 -68.502747  ... -59.474018 -63.432533   \n",
       "2 -66.885399 -66.885399 -66.885399 -66.885399  ... -64.676285 -59.603863   \n",
       "3 -54.603172 -55.583664 -54.093208 -53.048031  ... -52.305706 -54.098843   \n",
       "4 -57.124355 -56.951164 -56.631126 -56.283089  ... -53.750137 -56.681755   \n",
       "\n",
       "         209        210        211        212        213        214  \\\n",
       "0 -51.468235 -53.492142 -53.449692 -55.136826 -56.441799 -53.876381   \n",
       "1 -64.038330 -58.799248 -57.890312 -62.155491 -68.502747 -68.336723   \n",
       "2 -58.689316 -61.263062 -62.019516 -65.621262 -63.282192 -57.725925   \n",
       "3 -55.128151 -60.419147 -55.361618 -53.379673 -55.778790 -61.106613   \n",
       "4 -56.086063 -57.121132 -57.198044 -57.852798 -57.664429 -55.603500   \n",
       "\n",
       "         215             0    \n",
       "0 -53.726170    male_fearful  \n",
       "1 -68.385658     female_calm  \n",
       "2 -56.445858      female_sad  \n",
       "3 -61.522129      female_sad  \n",
       "4 -54.206291  female_fearful  \n",
       "\n",
       "[5 rows x 217 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnewdf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>-59.285912</td>\n",
       "      <td>-58.662746</td>\n",
       "      <td>-56.796532</td>\n",
       "      <td>-54.890263</td>\n",
       "      <td>-54.424412</td>\n",
       "      <td>-56.386520</td>\n",
       "      <td>-57.998100</td>\n",
       "      <td>-58.918205</td>\n",
       "      <td>-58.526035</td>\n",
       "      <td>-57.639275</td>\n",
       "      <td>...</td>\n",
       "      <td>-65.915726</td>\n",
       "      <td>-66.017220</td>\n",
       "      <td>-65.916924</td>\n",
       "      <td>-62.567467</td>\n",
       "      <td>-62.344139</td>\n",
       "      <td>-65.501350</td>\n",
       "      <td>-63.570717</td>\n",
       "      <td>-59.970089</td>\n",
       "      <td>-56.799137</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>-42.612179</td>\n",
       "      <td>-44.166237</td>\n",
       "      <td>-46.185032</td>\n",
       "      <td>-44.702591</td>\n",
       "      <td>-42.975319</td>\n",
       "      <td>-44.313457</td>\n",
       "      <td>-44.428078</td>\n",
       "      <td>-44.274414</td>\n",
       "      <td>-43.780632</td>\n",
       "      <td>-43.182186</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.133877</td>\n",
       "      <td>-39.412006</td>\n",
       "      <td>-37.711998</td>\n",
       "      <td>-39.044552</td>\n",
       "      <td>-38.289967</td>\n",
       "      <td>-38.845623</td>\n",
       "      <td>-40.895973</td>\n",
       "      <td>-42.518826</td>\n",
       "      <td>-43.199871</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>-37.781998</td>\n",
       "      <td>-37.955757</td>\n",
       "      <td>-41.867718</td>\n",
       "      <td>-43.668159</td>\n",
       "      <td>-43.195824</td>\n",
       "      <td>-43.210384</td>\n",
       "      <td>-44.112366</td>\n",
       "      <td>-45.227882</td>\n",
       "      <td>-45.010986</td>\n",
       "      <td>-44.672104</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.587936</td>\n",
       "      <td>-48.670307</td>\n",
       "      <td>-48.672791</td>\n",
       "      <td>-47.655327</td>\n",
       "      <td>-47.582119</td>\n",
       "      <td>-48.289742</td>\n",
       "      <td>-48.056358</td>\n",
       "      <td>-48.014286</td>\n",
       "      <td>-48.463039</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>-65.191589</td>\n",
       "      <td>-65.191589</td>\n",
       "      <td>-65.191589</td>\n",
       "      <td>-65.191589</td>\n",
       "      <td>-65.191589</td>\n",
       "      <td>-65.191589</td>\n",
       "      <td>-65.191589</td>\n",
       "      <td>-65.191589</td>\n",
       "      <td>-65.191589</td>\n",
       "      <td>-65.191589</td>\n",
       "      <td>...</td>\n",
       "      <td>-47.884926</td>\n",
       "      <td>-47.278912</td>\n",
       "      <td>-48.261562</td>\n",
       "      <td>-47.216820</td>\n",
       "      <td>-44.945267</td>\n",
       "      <td>-45.441326</td>\n",
       "      <td>-47.107605</td>\n",
       "      <td>-52.104263</td>\n",
       "      <td>-54.900394</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>-63.898258</td>\n",
       "      <td>-59.659367</td>\n",
       "      <td>-56.782402</td>\n",
       "      <td>-54.998432</td>\n",
       "      <td>-58.540775</td>\n",
       "      <td>-63.898258</td>\n",
       "      <td>-61.179314</td>\n",
       "      <td>-58.758705</td>\n",
       "      <td>-61.959270</td>\n",
       "      <td>-63.536812</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.474083</td>\n",
       "      <td>-58.214912</td>\n",
       "      <td>-58.713482</td>\n",
       "      <td>-58.827713</td>\n",
       "      <td>-57.919724</td>\n",
       "      <td>-57.178623</td>\n",
       "      <td>-57.815914</td>\n",
       "      <td>-62.274441</td>\n",
       "      <td>-63.898258</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>-45.871578</td>\n",
       "      <td>-45.871578</td>\n",
       "      <td>-45.871578</td>\n",
       "      <td>-45.871578</td>\n",
       "      <td>-45.871578</td>\n",
       "      <td>-45.871578</td>\n",
       "      <td>-45.871578</td>\n",
       "      <td>-45.871578</td>\n",
       "      <td>-45.871578</td>\n",
       "      <td>-45.871578</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.769211</td>\n",
       "      <td>-35.539146</td>\n",
       "      <td>-38.184052</td>\n",
       "      <td>-36.689140</td>\n",
       "      <td>-35.603161</td>\n",
       "      <td>-35.669231</td>\n",
       "      <td>-35.926704</td>\n",
       "      <td>-35.287487</td>\n",
       "      <td>-34.311398</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>-49.196129</td>\n",
       "      <td>-48.044453</td>\n",
       "      <td>-48.758469</td>\n",
       "      <td>-50.079197</td>\n",
       "      <td>-50.079197</td>\n",
       "      <td>-50.079197</td>\n",
       "      <td>-49.985477</td>\n",
       "      <td>-50.062176</td>\n",
       "      <td>-50.079197</td>\n",
       "      <td>-50.079197</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.324013</td>\n",
       "      <td>-21.457090</td>\n",
       "      <td>-22.004885</td>\n",
       "      <td>-23.449120</td>\n",
       "      <td>-23.008583</td>\n",
       "      <td>-24.557259</td>\n",
       "      <td>-24.817484</td>\n",
       "      <td>-23.378620</td>\n",
       "      <td>-21.804491</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>-61.080891</td>\n",
       "      <td>-61.470585</td>\n",
       "      <td>-61.351925</td>\n",
       "      <td>-59.571072</td>\n",
       "      <td>-59.312107</td>\n",
       "      <td>-59.351807</td>\n",
       "      <td>-62.197510</td>\n",
       "      <td>-61.885139</td>\n",
       "      <td>-59.451599</td>\n",
       "      <td>-61.568367</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.797737</td>\n",
       "      <td>-44.210587</td>\n",
       "      <td>-44.046528</td>\n",
       "      <td>-44.359676</td>\n",
       "      <td>-46.438553</td>\n",
       "      <td>-48.639870</td>\n",
       "      <td>-46.470398</td>\n",
       "      <td>-47.464104</td>\n",
       "      <td>-49.256027</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>-42.496696</td>\n",
       "      <td>-40.683514</td>\n",
       "      <td>-39.928291</td>\n",
       "      <td>-40.250160</td>\n",
       "      <td>-40.637547</td>\n",
       "      <td>-40.223694</td>\n",
       "      <td>-40.783562</td>\n",
       "      <td>-40.889061</td>\n",
       "      <td>-40.087566</td>\n",
       "      <td>-40.934109</td>\n",
       "      <td>...</td>\n",
       "      <td>-41.472267</td>\n",
       "      <td>-40.679695</td>\n",
       "      <td>-39.669472</td>\n",
       "      <td>-40.037292</td>\n",
       "      <td>-40.351273</td>\n",
       "      <td>-41.536926</td>\n",
       "      <td>-41.766766</td>\n",
       "      <td>-40.129128</td>\n",
       "      <td>-39.644192</td>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>-58.422295</td>\n",
       "      <td>-58.422295</td>\n",
       "      <td>-58.535202</td>\n",
       "      <td>-58.431622</td>\n",
       "      <td>-57.747078</td>\n",
       "      <td>-57.535259</td>\n",
       "      <td>-58.422295</td>\n",
       "      <td>-58.398277</td>\n",
       "      <td>-58.344955</td>\n",
       "      <td>-58.380268</td>\n",
       "      <td>...</td>\n",
       "      <td>-46.481236</td>\n",
       "      <td>-45.848122</td>\n",
       "      <td>-48.013786</td>\n",
       "      <td>-46.745735</td>\n",
       "      <td>-46.297077</td>\n",
       "      <td>-48.637226</td>\n",
       "      <td>-49.142132</td>\n",
       "      <td>-47.933002</td>\n",
       "      <td>-48.412838</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5    \\\n",
       "652 -59.285912 -58.662746 -56.796532 -54.890263 -54.424412 -56.386520   \n",
       "264 -42.612179 -44.166237 -46.185032 -44.702591 -42.975319 -44.313457   \n",
       "271 -37.781998 -37.955757 -41.867718 -43.668159 -43.195824 -43.210384   \n",
       "695 -65.191589 -65.191589 -65.191589 -65.191589 -65.191589 -65.191589   \n",
       "832 -63.898258 -59.659367 -56.782402 -54.998432 -58.540775 -63.898258   \n",
       "519 -45.871578 -45.871578 -45.871578 -45.871578 -45.871578 -45.871578   \n",
       "794 -49.196129 -48.044453 -48.758469 -50.079197 -50.079197 -50.079197   \n",
       "786 -61.080891 -61.470585 -61.351925 -59.571072 -59.312107 -59.351807   \n",
       "549 -42.496696 -40.683514 -39.928291 -40.250160 -40.637547 -40.223694   \n",
       "574 -58.422295 -58.422295 -58.535202 -58.431622 -57.747078 -57.535259   \n",
       "\n",
       "           6          7          8          9    ...        207        208  \\\n",
       "652 -57.998100 -58.918205 -58.526035 -57.639275  ... -65.915726 -66.017220   \n",
       "264 -44.428078 -44.274414 -43.780632 -43.182186  ... -39.133877 -39.412006   \n",
       "271 -44.112366 -45.227882 -45.010986 -44.672104  ... -48.587936 -48.670307   \n",
       "695 -65.191589 -65.191589 -65.191589 -65.191589  ... -47.884926 -47.278912   \n",
       "832 -61.179314 -58.758705 -61.959270 -63.536812  ... -56.474083 -58.214912   \n",
       "519 -45.871578 -45.871578 -45.871578 -45.871578  ... -34.769211 -35.539146   \n",
       "794 -49.985477 -50.062176 -50.079197 -50.079197  ... -21.324013 -21.457090   \n",
       "786 -62.197510 -61.885139 -59.451599 -61.568367  ... -44.797737 -44.210587   \n",
       "549 -40.783562 -40.889061 -40.087566 -40.934109  ... -41.472267 -40.679695   \n",
       "574 -58.422295 -58.398277 -58.344955 -58.380268  ... -46.481236 -45.848122   \n",
       "\n",
       "           209        210        211        212        213        214  \\\n",
       "652 -65.916924 -62.567467 -62.344139 -65.501350 -63.570717 -59.970089   \n",
       "264 -37.711998 -39.044552 -38.289967 -38.845623 -40.895973 -42.518826   \n",
       "271 -48.672791 -47.655327 -47.582119 -48.289742 -48.056358 -48.014286   \n",
       "695 -48.261562 -47.216820 -44.945267 -45.441326 -47.107605 -52.104263   \n",
       "832 -58.713482 -58.827713 -57.919724 -57.178623 -57.815914 -62.274441   \n",
       "519 -38.184052 -36.689140 -35.603161 -35.669231 -35.926704 -35.287487   \n",
       "794 -22.004885 -23.449120 -23.008583 -24.557259 -24.817484 -23.378620   \n",
       "786 -44.046528 -44.359676 -46.438553 -48.639870 -46.470398 -47.464104   \n",
       "549 -39.669472 -40.037292 -40.351273 -41.536926 -41.766766 -40.129128   \n",
       "574 -48.013786 -46.745735 -46.297077 -48.637226 -49.142132 -47.933002   \n",
       "\n",
       "           215             0    \n",
       "652 -56.799137        male_sad  \n",
       "264 -43.199871      male_happy  \n",
       "271 -48.463039    male_fearful  \n",
       "695 -54.900394     female_calm  \n",
       "832 -63.898258    male_fearful  \n",
       "519 -34.311398    female_angry  \n",
       "794 -21.804491      male_angry  \n",
       "786 -49.256027       male_calm  \n",
       "549 -39.644192  female_fearful  \n",
       "574 -48.412838    male_fearful  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "rnewdf = shuffle(newdf)\n",
    "rnewdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnewdf=rnewdf.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf1 = np.random.rand(len(rnewdf)) < 0.8\n",
    "train = rnewdf[newdf1]\n",
    "test = rnewdf[~newdf1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>-50.638081</td>\n",
       "      <td>-48.228836</td>\n",
       "      <td>-48.415012</td>\n",
       "      <td>-48.113029</td>\n",
       "      <td>-48.173935</td>\n",
       "      <td>-47.791752</td>\n",
       "      <td>-46.218651</td>\n",
       "      <td>-45.054356</td>\n",
       "      <td>-45.362675</td>\n",
       "      <td>-45.369656</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.810787</td>\n",
       "      <td>-26.735493</td>\n",
       "      <td>-28.539263</td>\n",
       "      <td>-31.688261</td>\n",
       "      <td>-31.474255</td>\n",
       "      <td>-31.985975</td>\n",
       "      <td>-32.624325</td>\n",
       "      <td>-35.602612</td>\n",
       "      <td>-36.271744</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>-63.131760</td>\n",
       "      <td>-61.888165</td>\n",
       "      <td>-59.611080</td>\n",
       "      <td>-57.938660</td>\n",
       "      <td>-61.089241</td>\n",
       "      <td>-60.685398</td>\n",
       "      <td>-62.597221</td>\n",
       "      <td>-68.623734</td>\n",
       "      <td>-69.007225</td>\n",
       "      <td>-66.853775</td>\n",
       "      <td>...</td>\n",
       "      <td>-63.206009</td>\n",
       "      <td>-60.798752</td>\n",
       "      <td>-60.709072</td>\n",
       "      <td>-56.783302</td>\n",
       "      <td>-55.975403</td>\n",
       "      <td>-61.550987</td>\n",
       "      <td>-63.805744</td>\n",
       "      <td>-60.823490</td>\n",
       "      <td>-57.963688</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>-57.737450</td>\n",
       "      <td>-59.686131</td>\n",
       "      <td>-57.828850</td>\n",
       "      <td>-55.253662</td>\n",
       "      <td>-56.984299</td>\n",
       "      <td>-56.965782</td>\n",
       "      <td>-57.298374</td>\n",
       "      <td>-57.570457</td>\n",
       "      <td>-57.718994</td>\n",
       "      <td>-58.874512</td>\n",
       "      <td>...</td>\n",
       "      <td>-51.479553</td>\n",
       "      <td>-53.759270</td>\n",
       "      <td>-54.388596</td>\n",
       "      <td>-55.250530</td>\n",
       "      <td>-54.296021</td>\n",
       "      <td>-53.732388</td>\n",
       "      <td>-55.841160</td>\n",
       "      <td>-57.228989</td>\n",
       "      <td>-60.722336</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-65.707649</td>\n",
       "      <td>-65.707649</td>\n",
       "      <td>-63.114719</td>\n",
       "      <td>-61.518997</td>\n",
       "      <td>-61.097134</td>\n",
       "      <td>-63.424595</td>\n",
       "      <td>-63.720058</td>\n",
       "      <td>-56.854614</td>\n",
       "      <td>-55.168972</td>\n",
       "      <td>-54.640007</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.792141</td>\n",
       "      <td>-40.613159</td>\n",
       "      <td>-41.209206</td>\n",
       "      <td>-41.439194</td>\n",
       "      <td>-43.994286</td>\n",
       "      <td>-49.399620</td>\n",
       "      <td>-50.591599</td>\n",
       "      <td>-49.144062</td>\n",
       "      <td>-48.705647</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>-63.790466</td>\n",
       "      <td>-63.947456</td>\n",
       "      <td>-63.355610</td>\n",
       "      <td>-63.286240</td>\n",
       "      <td>-63.246239</td>\n",
       "      <td>-62.802952</td>\n",
       "      <td>-62.743813</td>\n",
       "      <td>-62.665565</td>\n",
       "      <td>-62.665565</td>\n",
       "      <td>-62.665565</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.914062</td>\n",
       "      <td>-52.145931</td>\n",
       "      <td>-52.200073</td>\n",
       "      <td>-52.263309</td>\n",
       "      <td>-53.177227</td>\n",
       "      <td>-53.587490</td>\n",
       "      <td>-54.778591</td>\n",
       "      <td>-55.114746</td>\n",
       "      <td>-54.505997</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>-52.000729</td>\n",
       "      <td>-52.000729</td>\n",
       "      <td>-52.000729</td>\n",
       "      <td>-51.157795</td>\n",
       "      <td>-51.038548</td>\n",
       "      <td>-51.565109</td>\n",
       "      <td>-52.000729</td>\n",
       "      <td>-52.000729</td>\n",
       "      <td>-51.692673</td>\n",
       "      <td>-51.630722</td>\n",
       "      <td>...</td>\n",
       "      <td>-45.041477</td>\n",
       "      <td>-46.360538</td>\n",
       "      <td>-47.435749</td>\n",
       "      <td>-45.051735</td>\n",
       "      <td>-45.605301</td>\n",
       "      <td>-45.434124</td>\n",
       "      <td>-45.584862</td>\n",
       "      <td>-44.438187</td>\n",
       "      <td>-43.538654</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.762047</td>\n",
       "      <td>-52.983219</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>-54.475822</td>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>-52.543320</td>\n",
       "      <td>-52.900917</td>\n",
       "      <td>-50.362530</td>\n",
       "      <td>-46.141300</td>\n",
       "      <td>-48.566555</td>\n",
       "      <td>-50.858135</td>\n",
       "      <td>-52.272621</td>\n",
       "      <td>-49.521519</td>\n",
       "      <td>-47.651199</td>\n",
       "      <td>-45.625748</td>\n",
       "      <td>...</td>\n",
       "      <td>-47.986832</td>\n",
       "      <td>-49.680180</td>\n",
       "      <td>-52.731979</td>\n",
       "      <td>-51.756145</td>\n",
       "      <td>-51.770199</td>\n",
       "      <td>-53.004887</td>\n",
       "      <td>-53.497555</td>\n",
       "      <td>-53.787411</td>\n",
       "      <td>-52.571815</td>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>-63.509113</td>\n",
       "      <td>-63.633812</td>\n",
       "      <td>-63.504398</td>\n",
       "      <td>-63.299423</td>\n",
       "      <td>-63.838970</td>\n",
       "      <td>-65.177299</td>\n",
       "      <td>-66.479500</td>\n",
       "      <td>-65.160202</td>\n",
       "      <td>-65.833740</td>\n",
       "      <td>-64.724396</td>\n",
       "      <td>...</td>\n",
       "      <td>-40.389946</td>\n",
       "      <td>-36.990444</td>\n",
       "      <td>-38.102470</td>\n",
       "      <td>-40.782669</td>\n",
       "      <td>-40.174320</td>\n",
       "      <td>-40.962994</td>\n",
       "      <td>-43.929958</td>\n",
       "      <td>-43.562027</td>\n",
       "      <td>-41.383232</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>-59.806988</td>\n",
       "      <td>-61.757298</td>\n",
       "      <td>-66.645981</td>\n",
       "      <td>-65.127586</td>\n",
       "      <td>-62.448807</td>\n",
       "      <td>-61.828403</td>\n",
       "      <td>-62.077156</td>\n",
       "      <td>-66.091736</td>\n",
       "      <td>-66.174927</td>\n",
       "      <td>-59.648991</td>\n",
       "      <td>...</td>\n",
       "      <td>-42.003273</td>\n",
       "      <td>-43.790825</td>\n",
       "      <td>-38.334808</td>\n",
       "      <td>-31.672123</td>\n",
       "      <td>-30.431236</td>\n",
       "      <td>-28.434568</td>\n",
       "      <td>-26.703886</td>\n",
       "      <td>-26.550127</td>\n",
       "      <td>-24.701975</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5    \\\n",
       "689 -50.638081 -48.228836 -48.415012 -48.113029 -48.173935 -47.791752   \n",
       "746 -63.131760 -61.888165 -59.611080 -57.938660 -61.089241 -60.685398   \n",
       "815 -57.737450 -59.686131 -57.828850 -55.253662 -56.984299 -56.965782   \n",
       "115 -65.707649 -65.707649 -63.114719 -61.518997 -61.097134 -63.424595   \n",
       "807 -63.790466 -63.947456 -63.355610 -63.286240 -63.246239 -62.802952   \n",
       "637 -52.000729 -52.000729 -52.000729 -51.157795 -51.038548 -51.565109   \n",
       "728 -54.475822 -54.475822 -54.475822 -54.475822 -54.475822 -54.475822   \n",
       "680 -52.543320 -52.900917 -50.362530 -46.141300 -48.566555 -50.858135   \n",
       "388 -63.509113 -63.633812 -63.504398 -63.299423 -63.838970 -65.177299   \n",
       "722 -59.806988 -61.757298 -66.645981 -65.127586 -62.448807 -61.828403   \n",
       "\n",
       "           6          7          8          9    ...        207        208  \\\n",
       "689 -46.218651 -45.054356 -45.362675 -45.369656  ... -26.810787 -26.735493   \n",
       "746 -62.597221 -68.623734 -69.007225 -66.853775  ... -63.206009 -60.798752   \n",
       "815 -57.298374 -57.570457 -57.718994 -58.874512  ... -51.479553 -53.759270   \n",
       "115 -63.720058 -56.854614 -55.168972 -54.640007  ... -39.792141 -40.613159   \n",
       "807 -62.743813 -62.665565 -62.665565 -62.665565  ... -53.914062 -52.145931   \n",
       "637 -52.000729 -52.000729 -51.692673 -51.630722  ... -45.041477 -46.360538   \n",
       "728 -54.475822 -54.475822 -54.475822 -54.475822  ... -53.762047 -52.983219   \n",
       "680 -52.272621 -49.521519 -47.651199 -45.625748  ... -47.986832 -49.680180   \n",
       "388 -66.479500 -65.160202 -65.833740 -64.724396  ... -40.389946 -36.990444   \n",
       "722 -62.077156 -66.091736 -66.174927 -59.648991  ... -42.003273 -43.790825   \n",
       "\n",
       "           209        210        211        212        213        214  \\\n",
       "689 -28.539263 -31.688261 -31.474255 -31.985975 -32.624325 -35.602612   \n",
       "746 -60.709072 -56.783302 -55.975403 -61.550987 -63.805744 -60.823490   \n",
       "815 -54.388596 -55.250530 -54.296021 -53.732388 -55.841160 -57.228989   \n",
       "115 -41.209206 -41.439194 -43.994286 -49.399620 -50.591599 -49.144062   \n",
       "807 -52.200073 -52.263309 -53.177227 -53.587490 -54.778591 -55.114746   \n",
       "637 -47.435749 -45.051735 -45.605301 -45.434124 -45.584862 -44.438187   \n",
       "728 -54.475822 -54.475822 -54.475822 -54.475822 -54.475822 -54.475822   \n",
       "680 -52.731979 -51.756145 -51.770199 -53.004887 -53.497555 -53.787411   \n",
       "388 -38.102470 -40.782669 -40.174320 -40.962994 -43.929958 -43.562027   \n",
       "722 -38.334808 -31.672123 -30.431236 -28.434568 -26.703886 -26.550127   \n",
       "\n",
       "           215             0    \n",
       "689 -36.271744      male_happy  \n",
       "746 -57.963688       male_calm  \n",
       "815 -60.722336      female_sad  \n",
       "115 -48.705647     female_calm  \n",
       "807 -54.505997      female_sad  \n",
       "637 -43.538654      male_happy  \n",
       "728 -54.475822  female_fearful  \n",
       "680 -52.571815    female_happy  \n",
       "388 -41.383232       male_calm  \n",
       "722 -24.701975        male_sad  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[250:260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeatures = train.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabel = train.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfeatures = test.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlabel = test.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kanika/anaconda3/envs/minor/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train = np.array(trainfeatures)\n",
    "y_train = np.array(trainlabel)\n",
    "X_test = np.array(testfeatures)\n",
    "y_test = np.array(testlabel)\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(773, 216)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing dimension for CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_traincnn =np.expand_dims(X_train, axis=2)\n",
    "x_testcnn= np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kanika/anaconda3/envs/minor/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(256, 5,padding='same',\n",
    "                 input_shape=(216,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Conv1D(128, 5,padding='same',))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Conv1D(128, 5,padding='same',))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 216, 256)          1536      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 216, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 216, 128)          163968    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                34570     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 364,170\n",
      "Trainable params: 364,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removed the whole training part for avoiding unnecessary long epochs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kanika/anaconda3/envs/minor/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 773 samples, validate on 187 samples\n",
      "Epoch 1/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.4006 - accuracy: 0.1087 - val_loss: 2.2705 - val_accuracy: 0.1337\n",
      "Epoch 2/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.2517 - accuracy: 0.1591 - val_loss: 2.2449 - val_accuracy: 0.2139\n",
      "Epoch 3/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.2070 - accuracy: 0.1928 - val_loss: 2.2428 - val_accuracy: 0.1390\n",
      "Epoch 4/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.1916 - accuracy: 0.2122 - val_loss: 2.1993 - val_accuracy: 0.2139\n",
      "Epoch 5/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.1662 - accuracy: 0.2147 - val_loss: 2.1924 - val_accuracy: 0.1979\n",
      "Epoch 6/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.1480 - accuracy: 0.2380 - val_loss: 2.1653 - val_accuracy: 0.2086\n",
      "Epoch 7/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.1241 - accuracy: 0.2549 - val_loss: 2.1830 - val_accuracy: 0.1444\n",
      "Epoch 8/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.1080 - accuracy: 0.2419 - val_loss: 2.1509 - val_accuracy: 0.2139\n",
      "Epoch 9/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.0877 - accuracy: 0.2536 - val_loss: 2.1286 - val_accuracy: 0.2620\n",
      "Epoch 10/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.0679 - accuracy: 0.2872 - val_loss: 2.1383 - val_accuracy: 0.1551\n",
      "Epoch 11/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.0636 - accuracy: 0.2652 - val_loss: 2.1127 - val_accuracy: 0.2727\n",
      "Epoch 12/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.0373 - accuracy: 0.2924 - val_loss: 2.1033 - val_accuracy: 0.1979\n",
      "Epoch 13/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.0180 - accuracy: 0.2975 - val_loss: 2.1188 - val_accuracy: 0.1925\n",
      "Epoch 14/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 2.0109 - accuracy: 0.2859 - val_loss: 2.0783 - val_accuracy: 0.2674\n",
      "Epoch 15/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.9940 - accuracy: 0.2872 - val_loss: 2.0667 - val_accuracy: 0.2193\n",
      "Epoch 16/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.9748 - accuracy: 0.3040 - val_loss: 2.0369 - val_accuracy: 0.3155\n",
      "Epoch 17/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.9680 - accuracy: 0.3066 - val_loss: 2.0365 - val_accuracy: 0.2781\n",
      "Epoch 18/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.9469 - accuracy: 0.3027 - val_loss: 2.0334 - val_accuracy: 0.2513\n",
      "Epoch 19/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.9309 - accuracy: 0.3286 - val_loss: 1.9960 - val_accuracy: 0.2941\n",
      "Epoch 20/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.9182 - accuracy: 0.3364 - val_loss: 2.0045 - val_accuracy: 0.2513\n",
      "Epoch 21/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.9053 - accuracy: 0.3144 - val_loss: 2.0043 - val_accuracy: 0.2246\n",
      "Epoch 22/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.8885 - accuracy: 0.3260 - val_loss: 1.9889 - val_accuracy: 0.2353\n",
      "Epoch 23/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.8805 - accuracy: 0.3389 - val_loss: 1.9662 - val_accuracy: 0.2888\n",
      "Epoch 24/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.8654 - accuracy: 0.3622 - val_loss: 1.9809 - val_accuracy: 0.2139\n",
      "Epoch 25/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.8525 - accuracy: 0.3364 - val_loss: 1.9367 - val_accuracy: 0.3369\n",
      "Epoch 26/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.8450 - accuracy: 0.3519 - val_loss: 1.9439 - val_accuracy: 0.2406\n",
      "Epoch 27/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.8271 - accuracy: 0.3545 - val_loss: 1.9191 - val_accuracy: 0.3476\n",
      "Epoch 28/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.8225 - accuracy: 0.3441 - val_loss: 1.9169 - val_accuracy: 0.3209\n",
      "Epoch 29/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.8106 - accuracy: 0.3506 - val_loss: 1.9177 - val_accuracy: 0.2727\n",
      "Epoch 30/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.7951 - accuracy: 0.3777 - val_loss: 1.9037 - val_accuracy: 0.2995\n",
      "Epoch 31/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.7906 - accuracy: 0.3739 - val_loss: 1.9063 - val_accuracy: 0.2513\n",
      "Epoch 32/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.7797 - accuracy: 0.3752 - val_loss: 1.8726 - val_accuracy: 0.3369\n",
      "Epoch 33/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.7658 - accuracy: 0.3790 - val_loss: 1.8799 - val_accuracy: 0.3369\n",
      "Epoch 34/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.7614 - accuracy: 0.3752 - val_loss: 1.8862 - val_accuracy: 0.3316\n",
      "Epoch 35/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.7498 - accuracy: 0.3894 - val_loss: 1.8818 - val_accuracy: 0.2513\n",
      "Epoch 36/700\n",
      "773/773 [==============================] - 7s 8ms/step - loss: 1.7434 - accuracy: 0.3765 - val_loss: 1.8976 - val_accuracy: 0.2888\n",
      "Epoch 37/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.7312 - accuracy: 0.3868 - val_loss: 1.8512 - val_accuracy: 0.2995\n",
      "Epoch 38/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.7246 - accuracy: 0.3713 - val_loss: 1.8463 - val_accuracy: 0.3316\n",
      "Epoch 39/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.7170 - accuracy: 0.3933 - val_loss: 1.8440 - val_accuracy: 0.3690\n",
      "Epoch 40/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.7029 - accuracy: 0.4127 - val_loss: 1.8388 - val_accuracy: 0.2834\n",
      "Epoch 41/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.6879 - accuracy: 0.4062 - val_loss: 1.8578 - val_accuracy: 0.3102\n",
      "Epoch 42/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.6969 - accuracy: 0.3907 - val_loss: 1.8116 - val_accuracy: 0.3316\n",
      "Epoch 43/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.6809 - accuracy: 0.4088 - val_loss: 1.8097 - val_accuracy: 0.3048\n",
      "Epoch 44/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.6683 - accuracy: 0.4075 - val_loss: 1.8056 - val_accuracy: 0.3048\n",
      "Epoch 45/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.6616 - accuracy: 0.3907 - val_loss: 1.7894 - val_accuracy: 0.3369\n",
      "Epoch 46/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.6514 - accuracy: 0.4204 - val_loss: 1.8234 - val_accuracy: 0.2995\n",
      "Epoch 47/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.6453 - accuracy: 0.4062 - val_loss: 1.7941 - val_accuracy: 0.3422\n",
      "Epoch 48/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.6408 - accuracy: 0.4049 - val_loss: 1.8061 - val_accuracy: 0.2995\n",
      "Epoch 49/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.6336 - accuracy: 0.4217 - val_loss: 1.7768 - val_accuracy: 0.3316\n",
      "Epoch 50/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.6163 - accuracy: 0.4321 - val_loss: 1.8179 - val_accuracy: 0.3262\n",
      "Epoch 51/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.6140 - accuracy: 0.4282 - val_loss: 1.7817 - val_accuracy: 0.3155\n",
      "Epoch 52/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.6072 - accuracy: 0.4373 - val_loss: 1.7618 - val_accuracy: 0.3476\n",
      "Epoch 53/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.5906 - accuracy: 0.4386 - val_loss: 1.7526 - val_accuracy: 0.3476\n",
      "Epoch 54/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.5904 - accuracy: 0.4230 - val_loss: 1.7471 - val_accuracy: 0.3262\n",
      "Epoch 55/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.5870 - accuracy: 0.4463 - val_loss: 1.7512 - val_accuracy: 0.3369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.5791 - accuracy: 0.4347 - val_loss: 1.7380 - val_accuracy: 0.3583\n",
      "Epoch 57/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.5684 - accuracy: 0.4373 - val_loss: 1.7452 - val_accuracy: 0.3316\n",
      "Epoch 58/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.5648 - accuracy: 0.4541 - val_loss: 1.7366 - val_accuracy: 0.3155\n",
      "Epoch 59/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.5529 - accuracy: 0.4476 - val_loss: 1.7486 - val_accuracy: 0.3422\n",
      "Epoch 60/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.5451 - accuracy: 0.4554 - val_loss: 1.7231 - val_accuracy: 0.3690\n",
      "Epoch 61/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.5477 - accuracy: 0.4360 - val_loss: 1.7129 - val_accuracy: 0.3422\n",
      "Epoch 62/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.5263 - accuracy: 0.4567 - val_loss: 1.7172 - val_accuracy: 0.3262\n",
      "Epoch 63/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.5264 - accuracy: 0.4567 - val_loss: 1.7075 - val_accuracy: 0.3529\n",
      "Epoch 64/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.5208 - accuracy: 0.4657 - val_loss: 1.6981 - val_accuracy: 0.4118\n",
      "Epoch 65/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.5082 - accuracy: 0.4424 - val_loss: 1.6792 - val_accuracy: 0.3957\n",
      "Epoch 66/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4946 - accuracy: 0.4554 - val_loss: 1.6866 - val_accuracy: 0.3636\n",
      "Epoch 67/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4979 - accuracy: 0.4644 - val_loss: 1.7267 - val_accuracy: 0.3797\n",
      "Epoch 68/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4853 - accuracy: 0.4580 - val_loss: 1.6581 - val_accuracy: 0.3850\n",
      "Epoch 69/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4831 - accuracy: 0.4657 - val_loss: 1.6554 - val_accuracy: 0.3957\n",
      "Epoch 70/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4694 - accuracy: 0.4722 - val_loss: 1.7069 - val_accuracy: 0.3583\n",
      "Epoch 71/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4695 - accuracy: 0.4618 - val_loss: 1.6572 - val_accuracy: 0.4118\n",
      "Epoch 72/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4682 - accuracy: 0.4618 - val_loss: 1.6540 - val_accuracy: 0.3529\n",
      "Epoch 73/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4483 - accuracy: 0.4864 - val_loss: 1.7067 - val_accuracy: 0.3262\n",
      "Epoch 74/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4421 - accuracy: 0.4799 - val_loss: 1.6714 - val_accuracy: 0.3476\n",
      "Epoch 75/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4436 - accuracy: 0.4916 - val_loss: 1.6848 - val_accuracy: 0.3209\n",
      "Epoch 76/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4458 - accuracy: 0.4541 - val_loss: 1.6290 - val_accuracy: 0.4118\n",
      "Epoch 77/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4259 - accuracy: 0.4799 - val_loss: 1.6347 - val_accuracy: 0.3850\n",
      "Epoch 78/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4332 - accuracy: 0.4670 - val_loss: 1.6415 - val_accuracy: 0.4064\n",
      "Epoch 79/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4233 - accuracy: 0.4877 - val_loss: 1.7066 - val_accuracy: 0.2995\n",
      "Epoch 80/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4215 - accuracy: 0.4592 - val_loss: 1.6337 - val_accuracy: 0.3476\n",
      "Epoch 81/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4161 - accuracy: 0.4825 - val_loss: 1.6292 - val_accuracy: 0.3957\n",
      "Epoch 82/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3967 - accuracy: 0.4787 - val_loss: 1.6350 - val_accuracy: 0.3583\n",
      "Epoch 83/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4006 - accuracy: 0.4877 - val_loss: 1.6803 - val_accuracy: 0.3529\n",
      "Epoch 84/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.4002 - accuracy: 0.4903 - val_loss: 1.6208 - val_accuracy: 0.3797\n",
      "Epoch 85/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3978 - accuracy: 0.5045 - val_loss: 1.6497 - val_accuracy: 0.3690\n",
      "Epoch 86/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3853 - accuracy: 0.5019 - val_loss: 1.6083 - val_accuracy: 0.4118\n",
      "Epoch 87/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3904 - accuracy: 0.4851 - val_loss: 1.6076 - val_accuracy: 0.3904\n",
      "Epoch 88/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3808 - accuracy: 0.4981 - val_loss: 1.6128 - val_accuracy: 0.3743\n",
      "Epoch 89/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3695 - accuracy: 0.5071 - val_loss: 1.6085 - val_accuracy: 0.3690\n",
      "Epoch 90/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3663 - accuracy: 0.4877 - val_loss: 1.5956 - val_accuracy: 0.4011\n",
      "Epoch 91/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3487 - accuracy: 0.5136 - val_loss: 1.5883 - val_accuracy: 0.3850\n",
      "Epoch 92/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3574 - accuracy: 0.5175 - val_loss: 1.5681 - val_accuracy: 0.4171\n",
      "Epoch 93/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3533 - accuracy: 0.4877 - val_loss: 1.5800 - val_accuracy: 0.4064\n",
      "Epoch 94/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3539 - accuracy: 0.5110 - val_loss: 1.5685 - val_accuracy: 0.4011\n",
      "Epoch 95/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3482 - accuracy: 0.5084 - val_loss: 1.5665 - val_accuracy: 0.3743\n",
      "Epoch 96/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3312 - accuracy: 0.5006 - val_loss: 1.6193 - val_accuracy: 0.3529\n",
      "Epoch 97/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3289 - accuracy: 0.5071 - val_loss: 1.6062 - val_accuracy: 0.3529\n",
      "Epoch 98/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3244 - accuracy: 0.5123 - val_loss: 1.5952 - val_accuracy: 0.3957\n",
      "Epoch 99/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3247 - accuracy: 0.5019 - val_loss: 1.5849 - val_accuracy: 0.3850\n",
      "Epoch 100/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3313 - accuracy: 0.5058 - val_loss: 1.5752 - val_accuracy: 0.3636\n",
      "Epoch 101/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3180 - accuracy: 0.5265 - val_loss: 1.6170 - val_accuracy: 0.4011\n",
      "Epoch 102/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3184 - accuracy: 0.5136 - val_loss: 1.5687 - val_accuracy: 0.4225\n",
      "Epoch 103/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3064 - accuracy: 0.5369 - val_loss: 1.5645 - val_accuracy: 0.4118\n",
      "Epoch 104/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3084 - accuracy: 0.5149 - val_loss: 1.5917 - val_accuracy: 0.3583\n",
      "Epoch 105/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3089 - accuracy: 0.5162 - val_loss: 1.5841 - val_accuracy: 0.4011\n",
      "Epoch 106/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.3020 - accuracy: 0.5226 - val_loss: 1.5612 - val_accuracy: 0.4171\n",
      "Epoch 107/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2924 - accuracy: 0.5291 - val_loss: 1.5461 - val_accuracy: 0.3957\n",
      "Epoch 108/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.2864 - accuracy: 0.5356 - val_loss: 1.5428 - val_accuracy: 0.4385\n",
      "Epoch 109/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.2947 - accuracy: 0.5032 - val_loss: 1.5526 - val_accuracy: 0.3690\n",
      "Epoch 110/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2825 - accuracy: 0.5395 - val_loss: 1.5745 - val_accuracy: 0.4011\n",
      "Epoch 111/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2784 - accuracy: 0.5408 - val_loss: 1.6249 - val_accuracy: 0.3476\n",
      "Epoch 112/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2783 - accuracy: 0.5226 - val_loss: 1.6196 - val_accuracy: 0.3690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2695 - accuracy: 0.5252 - val_loss: 1.5264 - val_accuracy: 0.3850\n",
      "Epoch 114/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2697 - accuracy: 0.5278 - val_loss: 1.6057 - val_accuracy: 0.3743\n",
      "Epoch 115/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2650 - accuracy: 0.5420 - val_loss: 1.5505 - val_accuracy: 0.4118\n",
      "Epoch 116/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2634 - accuracy: 0.5330 - val_loss: 1.5945 - val_accuracy: 0.3529\n",
      "Epoch 117/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2641 - accuracy: 0.5433 - val_loss: 1.5354 - val_accuracy: 0.4064\n",
      "Epoch 118/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2581 - accuracy: 0.5408 - val_loss: 1.5462 - val_accuracy: 0.4064\n",
      "Epoch 119/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2529 - accuracy: 0.5524 - val_loss: 1.5733 - val_accuracy: 0.3690\n",
      "Epoch 120/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2564 - accuracy: 0.5446 - val_loss: 1.5730 - val_accuracy: 0.4225\n",
      "Epoch 121/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2542 - accuracy: 0.5278 - val_loss: 1.5908 - val_accuracy: 0.3690\n",
      "Epoch 122/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2475 - accuracy: 0.5343 - val_loss: 1.5584 - val_accuracy: 0.4332\n",
      "Epoch 123/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2338 - accuracy: 0.5408 - val_loss: 1.5171 - val_accuracy: 0.4278\n",
      "Epoch 124/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2500 - accuracy: 0.5433 - val_loss: 1.5845 - val_accuracy: 0.3690\n",
      "Epoch 125/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2416 - accuracy: 0.5382 - val_loss: 1.5204 - val_accuracy: 0.4225\n",
      "Epoch 126/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2304 - accuracy: 0.5576 - val_loss: 1.5382 - val_accuracy: 0.4118\n",
      "Epoch 127/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2320 - accuracy: 0.5524 - val_loss: 1.5721 - val_accuracy: 0.3957\n",
      "Epoch 128/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2275 - accuracy: 0.5356 - val_loss: 1.5637 - val_accuracy: 0.4064\n",
      "Epoch 129/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2227 - accuracy: 0.5692 - val_loss: 1.5962 - val_accuracy: 0.4011\n",
      "Epoch 130/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2202 - accuracy: 0.5602 - val_loss: 1.5348 - val_accuracy: 0.4278\n",
      "Epoch 131/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2187 - accuracy: 0.5382 - val_loss: 1.5650 - val_accuracy: 0.3797\n",
      "Epoch 132/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2182 - accuracy: 0.5744 - val_loss: 1.5299 - val_accuracy: 0.4118\n",
      "Epoch 133/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2115 - accuracy: 0.5653 - val_loss: 1.5777 - val_accuracy: 0.3957\n",
      "Epoch 134/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1995 - accuracy: 0.5614 - val_loss: 1.5883 - val_accuracy: 0.4171\n",
      "Epoch 135/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2040 - accuracy: 0.5705 - val_loss: 1.5891 - val_accuracy: 0.3690\n",
      "Epoch 136/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2075 - accuracy: 0.5589 - val_loss: 1.5232 - val_accuracy: 0.4171\n",
      "Epoch 137/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1991 - accuracy: 0.5576 - val_loss: 1.5712 - val_accuracy: 0.3904\n",
      "Epoch 138/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1992 - accuracy: 0.5550 - val_loss: 1.5659 - val_accuracy: 0.4385\n",
      "Epoch 139/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.2008 - accuracy: 0.5589 - val_loss: 1.5309 - val_accuracy: 0.4064\n",
      "Epoch 140/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1911 - accuracy: 0.5653 - val_loss: 1.5300 - val_accuracy: 0.4171\n",
      "Epoch 141/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1859 - accuracy: 0.5692 - val_loss: 1.5720 - val_accuracy: 0.4278\n",
      "Epoch 142/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1859 - accuracy: 0.5783 - val_loss: 1.6036 - val_accuracy: 0.3422\n",
      "Epoch 143/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1846 - accuracy: 0.5550 - val_loss: 1.5698 - val_accuracy: 0.3797\n",
      "Epoch 144/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1793 - accuracy: 0.5731 - val_loss: 1.5434 - val_accuracy: 0.4278\n",
      "Epoch 145/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1835 - accuracy: 0.5692 - val_loss: 1.5599 - val_accuracy: 0.3797\n",
      "Epoch 146/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1761 - accuracy: 0.5692 - val_loss: 1.5609 - val_accuracy: 0.4225\n",
      "Epoch 147/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1746 - accuracy: 0.5666 - val_loss: 1.5338 - val_accuracy: 0.3904\n",
      "Epoch 148/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1727 - accuracy: 0.5886 - val_loss: 1.5616 - val_accuracy: 0.4064\n",
      "Epoch 149/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1708 - accuracy: 0.5757 - val_loss: 1.5724 - val_accuracy: 0.4225\n",
      "Epoch 150/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1706 - accuracy: 0.5783 - val_loss: 1.5248 - val_accuracy: 0.4171\n",
      "Epoch 151/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.1634 - accuracy: 0.5744 - val_loss: 1.5433 - val_accuracy: 0.4118\n",
      "Epoch 152/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1661 - accuracy: 0.5809 - val_loss: 1.5205 - val_accuracy: 0.3850\n",
      "Epoch 153/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1626 - accuracy: 0.5705 - val_loss: 1.5180 - val_accuracy: 0.4064\n",
      "Epoch 154/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1573 - accuracy: 0.5692 - val_loss: 1.5157 - val_accuracy: 0.4118\n",
      "Epoch 155/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1583 - accuracy: 0.5783 - val_loss: 1.5377 - val_accuracy: 0.3797\n",
      "Epoch 156/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1457 - accuracy: 0.6067 - val_loss: 1.5592 - val_accuracy: 0.3850\n",
      "Epoch 157/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1554 - accuracy: 0.5899 - val_loss: 1.5436 - val_accuracy: 0.4011\n",
      "Epoch 158/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1440 - accuracy: 0.5925 - val_loss: 1.5144 - val_accuracy: 0.4385\n",
      "Epoch 159/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1338 - accuracy: 0.6016 - val_loss: 1.5703 - val_accuracy: 0.3583\n",
      "Epoch 160/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1483 - accuracy: 0.5834 - val_loss: 1.5124 - val_accuracy: 0.4385\n",
      "Epoch 161/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1334 - accuracy: 0.5809 - val_loss: 1.5359 - val_accuracy: 0.4118\n",
      "Epoch 162/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1331 - accuracy: 0.5951 - val_loss: 1.5289 - val_accuracy: 0.4118\n",
      "Epoch 163/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1351 - accuracy: 0.5886 - val_loss: 1.5901 - val_accuracy: 0.3797\n",
      "Epoch 164/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1292 - accuracy: 0.5938 - val_loss: 1.5479 - val_accuracy: 0.4064\n",
      "Epoch 165/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1308 - accuracy: 0.5977 - val_loss: 1.5312 - val_accuracy: 0.3957\n",
      "Epoch 166/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1332 - accuracy: 0.6041 - val_loss: 1.4950 - val_accuracy: 0.4225\n",
      "Epoch 167/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1188 - accuracy: 0.6119 - val_loss: 1.5603 - val_accuracy: 0.3904\n",
      "Epoch 168/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1314 - accuracy: 0.5821 - val_loss: 1.5001 - val_accuracy: 0.4278\n",
      "Epoch 169/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1265 - accuracy: 0.6210 - val_loss: 1.5781 - val_accuracy: 0.3850\n",
      "Epoch 170/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1259 - accuracy: 0.5821 - val_loss: 1.5004 - val_accuracy: 0.4492\n",
      "Epoch 171/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1268 - accuracy: 0.6016 - val_loss: 1.5069 - val_accuracy: 0.4225\n",
      "Epoch 172/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1098 - accuracy: 0.6067 - val_loss: 1.5544 - val_accuracy: 0.3636\n",
      "Epoch 173/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1157 - accuracy: 0.5938 - val_loss: 1.5303 - val_accuracy: 0.4492\n",
      "Epoch 174/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1094 - accuracy: 0.5964 - val_loss: 1.4914 - val_accuracy: 0.3957\n",
      "Epoch 175/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1058 - accuracy: 0.6003 - val_loss: 1.5218 - val_accuracy: 0.3904\n",
      "Epoch 176/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1186 - accuracy: 0.5912 - val_loss: 1.5035 - val_accuracy: 0.4171\n",
      "Epoch 177/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1017 - accuracy: 0.5925 - val_loss: 1.6005 - val_accuracy: 0.3636\n",
      "Epoch 178/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1080 - accuracy: 0.5990 - val_loss: 1.4691 - val_accuracy: 0.4118\n",
      "Epoch 179/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1011 - accuracy: 0.6093 - val_loss: 1.4834 - val_accuracy: 0.4225\n",
      "Epoch 180/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0958 - accuracy: 0.6041 - val_loss: 1.5234 - val_accuracy: 0.4545\n",
      "Epoch 181/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0998 - accuracy: 0.6003 - val_loss: 1.4875 - val_accuracy: 0.4545\n",
      "Epoch 182/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.1011 - accuracy: 0.5925 - val_loss: 1.5226 - val_accuracy: 0.4225\n",
      "Epoch 183/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0925 - accuracy: 0.5964 - val_loss: 1.5346 - val_accuracy: 0.4011\n",
      "Epoch 184/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0882 - accuracy: 0.6119 - val_loss: 1.5694 - val_accuracy: 0.3690\n",
      "Epoch 185/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0901 - accuracy: 0.6041 - val_loss: 1.5393 - val_accuracy: 0.4118\n",
      "Epoch 186/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0833 - accuracy: 0.6054 - val_loss: 1.5375 - val_accuracy: 0.4064\n",
      "Epoch 187/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0900 - accuracy: 0.6223 - val_loss: 1.4968 - val_accuracy: 0.3904\n",
      "Epoch 188/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0695 - accuracy: 0.6235 - val_loss: 1.4862 - val_accuracy: 0.4385\n",
      "Epoch 189/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0835 - accuracy: 0.6003 - val_loss: 1.5394 - val_accuracy: 0.4492\n",
      "Epoch 190/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0777 - accuracy: 0.6145 - val_loss: 1.5404 - val_accuracy: 0.3850\n",
      "Epoch 191/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0755 - accuracy: 0.5990 - val_loss: 1.5433 - val_accuracy: 0.4011\n",
      "Epoch 192/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0787 - accuracy: 0.6119 - val_loss: 1.5022 - val_accuracy: 0.4332\n",
      "Epoch 193/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0599 - accuracy: 0.6287 - val_loss: 1.5437 - val_accuracy: 0.3904\n",
      "Epoch 194/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0762 - accuracy: 0.5847 - val_loss: 1.5082 - val_accuracy: 0.3797\n",
      "Epoch 195/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0644 - accuracy: 0.6093 - val_loss: 1.5271 - val_accuracy: 0.4225\n",
      "Epoch 196/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0688 - accuracy: 0.6093 - val_loss: 1.5484 - val_accuracy: 0.4171\n",
      "Epoch 197/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0655 - accuracy: 0.6106 - val_loss: 1.4985 - val_accuracy: 0.4011\n",
      "Epoch 198/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0632 - accuracy: 0.6093 - val_loss: 1.5116 - val_accuracy: 0.4118\n",
      "Epoch 199/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0559 - accuracy: 0.6223 - val_loss: 1.4764 - val_accuracy: 0.4385\n",
      "Epoch 200/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0574 - accuracy: 0.6171 - val_loss: 1.5158 - val_accuracy: 0.4118\n",
      "Epoch 201/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 1.0626 - accuracy: 0.6132 - val_loss: 1.4838 - val_accuracy: 0.4492\n",
      "Epoch 202/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0557 - accuracy: 0.6197 - val_loss: 1.5255 - val_accuracy: 0.4011\n",
      "Epoch 203/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0561 - accuracy: 0.6287 - val_loss: 1.4914 - val_accuracy: 0.4385\n",
      "Epoch 204/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0507 - accuracy: 0.6145 - val_loss: 1.5171 - val_accuracy: 0.4171\n",
      "Epoch 205/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0434 - accuracy: 0.6248 - val_loss: 1.5126 - val_accuracy: 0.4332\n",
      "Epoch 206/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0486 - accuracy: 0.6210 - val_loss: 1.4821 - val_accuracy: 0.4011\n",
      "Epoch 207/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0291 - accuracy: 0.6184 - val_loss: 1.4985 - val_accuracy: 0.4225\n",
      "Epoch 208/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0402 - accuracy: 0.6365 - val_loss: 1.5352 - val_accuracy: 0.4225\n",
      "Epoch 209/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0512 - accuracy: 0.6106 - val_loss: 1.4855 - val_accuracy: 0.4064\n",
      "Epoch 210/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0391 - accuracy: 0.6197 - val_loss: 1.5336 - val_accuracy: 0.4064\n",
      "Epoch 211/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0452 - accuracy: 0.6274 - val_loss: 1.4824 - val_accuracy: 0.4171\n",
      "Epoch 212/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0322 - accuracy: 0.6365 - val_loss: 1.5016 - val_accuracy: 0.4171\n",
      "Epoch 213/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0249 - accuracy: 0.6274 - val_loss: 1.5298 - val_accuracy: 0.3904\n",
      "Epoch 214/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0313 - accuracy: 0.6365 - val_loss: 1.5072 - val_accuracy: 0.4332\n",
      "Epoch 215/700\n",
      "773/773 [==============================] - 7s 10ms/step - loss: 1.0345 - accuracy: 0.6235 - val_loss: 1.5554 - val_accuracy: 0.4118\n",
      "Epoch 216/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0126 - accuracy: 0.6391 - val_loss: 1.5745 - val_accuracy: 0.3904\n",
      "Epoch 217/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0272 - accuracy: 0.6417 - val_loss: 1.5103 - val_accuracy: 0.4225\n",
      "Epoch 218/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0277 - accuracy: 0.6326 - val_loss: 1.4962 - val_accuracy: 0.4439\n",
      "Epoch 219/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0202 - accuracy: 0.6365 - val_loss: 1.4976 - val_accuracy: 0.4385\n",
      "Epoch 220/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0083 - accuracy: 0.6391 - val_loss: 1.5535 - val_accuracy: 0.3743\n",
      "Epoch 221/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0056 - accuracy: 0.6417 - val_loss: 1.4990 - val_accuracy: 0.4439\n",
      "Epoch 222/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0125 - accuracy: 0.6223 - val_loss: 1.5026 - val_accuracy: 0.4439\n",
      "Epoch 223/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0058 - accuracy: 0.6313 - val_loss: 1.5224 - val_accuracy: 0.4225\n",
      "Epoch 224/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0117 - accuracy: 0.6326 - val_loss: 1.4777 - val_accuracy: 0.4332\n",
      "Epoch 225/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0122 - accuracy: 0.6261 - val_loss: 1.4868 - val_accuracy: 0.4385\n",
      "Epoch 226/700\n",
      "773/773 [==============================] - 9s 11ms/step - loss: 0.9965 - accuracy: 0.6287 - val_loss: 1.4687 - val_accuracy: 0.4439\n",
      "Epoch 227/700\n",
      "773/773 [==============================] - 8s 10ms/step - loss: 1.0064 - accuracy: 0.6455 - val_loss: 1.4969 - val_accuracy: 0.4225\n",
      "Epoch 228/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0000 - accuracy: 0.6507 - val_loss: 1.4746 - val_accuracy: 0.4278\n",
      "Epoch 229/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9986 - accuracy: 0.6326 - val_loss: 1.5526 - val_accuracy: 0.3636\n",
      "Epoch 230/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 1.0014 - accuracy: 0.6507 - val_loss: 1.5021 - val_accuracy: 0.4278\n",
      "Epoch 231/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9954 - accuracy: 0.6235 - val_loss: 1.5097 - val_accuracy: 0.4118\n",
      "Epoch 232/700\n",
      "773/773 [==============================] - 8s 10ms/step - loss: 0.9972 - accuracy: 0.6365 - val_loss: 1.5086 - val_accuracy: 0.4439\n",
      "Epoch 233/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9952 - accuracy: 0.6455 - val_loss: 1.5181 - val_accuracy: 0.4011\n",
      "Epoch 234/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9924 - accuracy: 0.6378 - val_loss: 1.5067 - val_accuracy: 0.4492\n",
      "Epoch 235/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9948 - accuracy: 0.6468 - val_loss: 1.5155 - val_accuracy: 0.4385\n",
      "Epoch 236/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9952 - accuracy: 0.6481 - val_loss: 1.5033 - val_accuracy: 0.4064\n",
      "Epoch 237/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9881 - accuracy: 0.6688 - val_loss: 1.4958 - val_accuracy: 0.4332\n",
      "Epoch 238/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9765 - accuracy: 0.6417 - val_loss: 1.5562 - val_accuracy: 0.4599\n",
      "Epoch 239/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9834 - accuracy: 0.6404 - val_loss: 1.4930 - val_accuracy: 0.4492\n",
      "Epoch 240/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9823 - accuracy: 0.6507 - val_loss: 1.4843 - val_accuracy: 0.4439\n",
      "Epoch 241/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9858 - accuracy: 0.6494 - val_loss: 1.5050 - val_accuracy: 0.4385\n",
      "Epoch 242/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9754 - accuracy: 0.6598 - val_loss: 1.5150 - val_accuracy: 0.4064\n",
      "Epoch 243/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9662 - accuracy: 0.6598 - val_loss: 1.4856 - val_accuracy: 0.4492\n",
      "Epoch 244/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9639 - accuracy: 0.6649 - val_loss: 1.5533 - val_accuracy: 0.3957\n",
      "Epoch 245/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9742 - accuracy: 0.6365 - val_loss: 1.5474 - val_accuracy: 0.4332\n",
      "Epoch 246/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9703 - accuracy: 0.6507 - val_loss: 1.5007 - val_accuracy: 0.4439\n",
      "Epoch 247/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9590 - accuracy: 0.6559 - val_loss: 1.5515 - val_accuracy: 0.4439\n",
      "Epoch 248/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9653 - accuracy: 0.6559 - val_loss: 1.6386 - val_accuracy: 0.3690\n",
      "Epoch 249/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9700 - accuracy: 0.6481 - val_loss: 1.5150 - val_accuracy: 0.4064\n",
      "Epoch 250/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9608 - accuracy: 0.6559 - val_loss: 1.5429 - val_accuracy: 0.4171\n",
      "Epoch 251/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9572 - accuracy: 0.6636 - val_loss: 1.5322 - val_accuracy: 0.4225\n",
      "Epoch 252/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9595 - accuracy: 0.6611 - val_loss: 1.4997 - val_accuracy: 0.4278\n",
      "Epoch 253/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9535 - accuracy: 0.6585 - val_loss: 1.4836 - val_accuracy: 0.4439\n",
      "Epoch 254/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9469 - accuracy: 0.6727 - val_loss: 1.5115 - val_accuracy: 0.4225\n",
      "Epoch 255/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9587 - accuracy: 0.6701 - val_loss: 1.5131 - val_accuracy: 0.4278\n",
      "Epoch 256/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9483 - accuracy: 0.6533 - val_loss: 1.5482 - val_accuracy: 0.4118\n",
      "Epoch 257/700\n",
      "773/773 [==============================] - 7s 10ms/step - loss: 0.9510 - accuracy: 0.6481 - val_loss: 1.4997 - val_accuracy: 0.4439\n",
      "Epoch 258/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9513 - accuracy: 0.6714 - val_loss: 1.5099 - val_accuracy: 0.4011\n",
      "Epoch 259/700\n",
      "773/773 [==============================] - 8s 10ms/step - loss: 0.9321 - accuracy: 0.6572 - val_loss: 1.5062 - val_accuracy: 0.4439\n",
      "Epoch 260/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9461 - accuracy: 0.6624 - val_loss: 1.5053 - val_accuracy: 0.4225\n",
      "Epoch 261/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9415 - accuracy: 0.6559 - val_loss: 1.4869 - val_accuracy: 0.4385\n",
      "Epoch 262/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9409 - accuracy: 0.6624 - val_loss: 1.5049 - val_accuracy: 0.4385\n",
      "Epoch 263/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9285 - accuracy: 0.6675 - val_loss: 1.5134 - val_accuracy: 0.4225\n",
      "Epoch 264/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9341 - accuracy: 0.6714 - val_loss: 1.4967 - val_accuracy: 0.3850\n",
      "Epoch 265/700\n",
      "773/773 [==============================] - 7s 10ms/step - loss: 0.9327 - accuracy: 0.6740 - val_loss: 1.5273 - val_accuracy: 0.4118\n",
      "Epoch 266/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9266 - accuracy: 0.6649 - val_loss: 1.4715 - val_accuracy: 0.4225\n",
      "Epoch 267/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9309 - accuracy: 0.6611 - val_loss: 1.5296 - val_accuracy: 0.4278\n",
      "Epoch 268/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9281 - accuracy: 0.6598 - val_loss: 1.5319 - val_accuracy: 0.4064\n",
      "Epoch 269/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9217 - accuracy: 0.6572 - val_loss: 1.5020 - val_accuracy: 0.4225\n",
      "Epoch 270/700\n",
      "773/773 [==============================] - 7s 10ms/step - loss: 0.9357 - accuracy: 0.6636 - val_loss: 1.4828 - val_accuracy: 0.4439\n",
      "Epoch 271/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9216 - accuracy: 0.6701 - val_loss: 1.5367 - val_accuracy: 0.3797\n",
      "Epoch 272/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9199 - accuracy: 0.6740 - val_loss: 1.5543 - val_accuracy: 0.4385\n",
      "Epoch 273/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9179 - accuracy: 0.6753 - val_loss: 1.5126 - val_accuracy: 0.3957\n",
      "Epoch 274/700\n",
      "773/773 [==============================] - 7s 10ms/step - loss: 0.9185 - accuracy: 0.6766 - val_loss: 1.5183 - val_accuracy: 0.4118\n",
      "Epoch 275/700\n",
      "773/773 [==============================] - 7s 10ms/step - loss: 0.9241 - accuracy: 0.6856 - val_loss: 1.4764 - val_accuracy: 0.4278\n",
      "Epoch 276/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9096 - accuracy: 0.6662 - val_loss: 1.5050 - val_accuracy: 0.4332\n",
      "Epoch 277/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9166 - accuracy: 0.6727 - val_loss: 1.5346 - val_accuracy: 0.4064\n",
      "Epoch 278/700\n",
      "773/773 [==============================] - 7s 10ms/step - loss: 0.9120 - accuracy: 0.6843 - val_loss: 1.5105 - val_accuracy: 0.4225\n",
      "Epoch 279/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9025 - accuracy: 0.6766 - val_loss: 1.5296 - val_accuracy: 0.4171\n",
      "Epoch 280/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9115 - accuracy: 0.6740 - val_loss: 1.4916 - val_accuracy: 0.4385\n",
      "Epoch 281/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9120 - accuracy: 0.6805 - val_loss: 1.5197 - val_accuracy: 0.4171\n",
      "Epoch 282/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9077 - accuracy: 0.6766 - val_loss: 1.5129 - val_accuracy: 0.3957\n",
      "Epoch 283/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9055 - accuracy: 0.6831 - val_loss: 1.4986 - val_accuracy: 0.4332\n",
      "Epoch 284/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.9030 - accuracy: 0.6753 - val_loss: 1.5498 - val_accuracy: 0.4332\n",
      "Epoch 285/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.8928 - accuracy: 0.6934 - val_loss: 1.5329 - val_accuracy: 0.4545\n",
      "Epoch 286/700\n",
      "773/773 [==============================] - 58s 75ms/step - loss: 0.8974 - accuracy: 0.6921 - val_loss: 1.5275 - val_accuracy: 0.3904\n",
      "Epoch 287/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8914 - accuracy: 0.6947 - val_loss: 1.4939 - val_accuracy: 0.4492\n",
      "Epoch 288/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8791 - accuracy: 0.7012 - val_loss: 1.5458 - val_accuracy: 0.4439\n",
      "Epoch 289/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.8889 - accuracy: 0.6856 - val_loss: 1.5377 - val_accuracy: 0.4439\n",
      "Epoch 290/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8868 - accuracy: 0.6960 - val_loss: 1.5031 - val_accuracy: 0.4385\n",
      "Epoch 291/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.8855 - accuracy: 0.6843 - val_loss: 1.5390 - val_accuracy: 0.4225\n",
      "Epoch 292/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8819 - accuracy: 0.6973 - val_loss: 1.5484 - val_accuracy: 0.4385\n",
      "Epoch 293/700\n",
      "773/773 [==============================] - 8s 10ms/step - loss: 0.8795 - accuracy: 0.7025 - val_loss: 1.5099 - val_accuracy: 0.4545\n",
      "Epoch 294/700\n",
      "773/773 [==============================] - 7s 8ms/step - loss: 0.8814 - accuracy: 0.6960 - val_loss: 1.4979 - val_accuracy: 0.4599\n",
      "Epoch 295/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8745 - accuracy: 0.6960 - val_loss: 1.5223 - val_accuracy: 0.4171\n",
      "Epoch 296/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8772 - accuracy: 0.6934 - val_loss: 1.5121 - val_accuracy: 0.4225\n",
      "Epoch 297/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8813 - accuracy: 0.6960 - val_loss: 1.4800 - val_accuracy: 0.4385\n",
      "Epoch 298/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8838 - accuracy: 0.6831 - val_loss: 1.4897 - val_accuracy: 0.4599\n",
      "Epoch 299/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8761 - accuracy: 0.6856 - val_loss: 1.5095 - val_accuracy: 0.4118\n",
      "Epoch 300/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8658 - accuracy: 0.6973 - val_loss: 1.5242 - val_accuracy: 0.4385\n",
      "Epoch 301/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8704 - accuracy: 0.6921 - val_loss: 1.5429 - val_accuracy: 0.4278\n",
      "Epoch 302/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.8661 - accuracy: 0.6999 - val_loss: 1.5307 - val_accuracy: 0.4492\n",
      "Epoch 303/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.8527 - accuracy: 0.6986 - val_loss: 1.5446 - val_accuracy: 0.4118\n",
      "Epoch 304/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8643 - accuracy: 0.6921 - val_loss: 1.5792 - val_accuracy: 0.3904\n",
      "Epoch 305/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8626 - accuracy: 0.6986 - val_loss: 1.5422 - val_accuracy: 0.4118\n",
      "Epoch 306/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8626 - accuracy: 0.6895 - val_loss: 1.5048 - val_accuracy: 0.4545\n",
      "Epoch 307/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8633 - accuracy: 0.6869 - val_loss: 1.5244 - val_accuracy: 0.4385\n",
      "Epoch 308/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8481 - accuracy: 0.7012 - val_loss: 1.5492 - val_accuracy: 0.4332\n",
      "Epoch 309/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8576 - accuracy: 0.7038 - val_loss: 1.5421 - val_accuracy: 0.4385\n",
      "Epoch 310/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8453 - accuracy: 0.6960 - val_loss: 1.6136 - val_accuracy: 0.4278\n",
      "Epoch 311/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8575 - accuracy: 0.6986 - val_loss: 1.5294 - val_accuracy: 0.4118\n",
      "Epoch 312/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8463 - accuracy: 0.7038 - val_loss: 1.5568 - val_accuracy: 0.4118\n",
      "Epoch 313/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8467 - accuracy: 0.7063 - val_loss: 1.5343 - val_accuracy: 0.4118\n",
      "Epoch 314/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8506 - accuracy: 0.6999 - val_loss: 1.5830 - val_accuracy: 0.4064\n",
      "Epoch 315/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8413 - accuracy: 0.7102 - val_loss: 1.5161 - val_accuracy: 0.4171\n",
      "Epoch 316/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8459 - accuracy: 0.7012 - val_loss: 1.5258 - val_accuracy: 0.3904\n",
      "Epoch 317/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8380 - accuracy: 0.7063 - val_loss: 1.5592 - val_accuracy: 0.4011\n",
      "Epoch 318/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8383 - accuracy: 0.7050 - val_loss: 1.5661 - val_accuracy: 0.4011\n",
      "Epoch 319/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8373 - accuracy: 0.7050 - val_loss: 1.5178 - val_accuracy: 0.4278\n",
      "Epoch 320/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8332 - accuracy: 0.7141 - val_loss: 1.5036 - val_accuracy: 0.4439\n",
      "Epoch 321/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8274 - accuracy: 0.7154 - val_loss: 1.5010 - val_accuracy: 0.4599\n",
      "Epoch 322/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8320 - accuracy: 0.7128 - val_loss: 1.5481 - val_accuracy: 0.4545\n",
      "Epoch 323/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8369 - accuracy: 0.7076 - val_loss: 1.5575 - val_accuracy: 0.4332\n",
      "Epoch 324/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8267 - accuracy: 0.7232 - val_loss: 1.5278 - val_accuracy: 0.4225\n",
      "Epoch 325/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8208 - accuracy: 0.7232 - val_loss: 1.5120 - val_accuracy: 0.4545\n",
      "Epoch 326/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.8275 - accuracy: 0.7154 - val_loss: 1.5369 - val_accuracy: 0.4439\n",
      "Epoch 327/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.8192 - accuracy: 0.7245 - val_loss: 1.5496 - val_accuracy: 0.4332\n",
      "Epoch 328/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8185 - accuracy: 0.7128 - val_loss: 1.5512 - val_accuracy: 0.4225\n",
      "Epoch 329/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.8156 - accuracy: 0.7296 - val_loss: 1.6069 - val_accuracy: 0.4011\n",
      "Epoch 330/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.8217 - accuracy: 0.7038 - val_loss: 1.5508 - val_accuracy: 0.4064\n",
      "Epoch 331/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.8170 - accuracy: 0.7154 - val_loss: 1.5688 - val_accuracy: 0.4439\n",
      "Epoch 332/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.8055 - accuracy: 0.7141 - val_loss: 1.5123 - val_accuracy: 0.4652\n",
      "Epoch 333/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.8099 - accuracy: 0.7245 - val_loss: 1.5234 - val_accuracy: 0.4171\n",
      "Epoch 334/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.8095 - accuracy: 0.7270 - val_loss: 1.5140 - val_accuracy: 0.4385\n",
      "Epoch 335/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.8039 - accuracy: 0.7270 - val_loss: 1.5183 - val_accuracy: 0.4439\n",
      "Epoch 336/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.8020 - accuracy: 0.7219 - val_loss: 1.6077 - val_accuracy: 0.4011\n",
      "Epoch 337/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/773 [==============================] - 6s 7ms/step - loss: 0.8077 - accuracy: 0.7141 - val_loss: 1.5360 - val_accuracy: 0.4492\n",
      "Epoch 338/700\n",
      "773/773 [==============================] - 8s 10ms/step - loss: 0.8015 - accuracy: 0.7348 - val_loss: 1.5450 - val_accuracy: 0.4599\n",
      "Epoch 339/700\n",
      "773/773 [==============================] - 10s 13ms/step - loss: 0.7973 - accuracy: 0.7374 - val_loss: 1.5635 - val_accuracy: 0.3850\n",
      "Epoch 340/700\n",
      "773/773 [==============================] - 10s 12ms/step - loss: 0.7998 - accuracy: 0.7309 - val_loss: 1.5170 - val_accuracy: 0.4439\n",
      "Epoch 341/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.8013 - accuracy: 0.7128 - val_loss: 1.5573 - val_accuracy: 0.4118\n",
      "Epoch 342/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7957 - accuracy: 0.7089 - val_loss: 1.6271 - val_accuracy: 0.3743\n",
      "Epoch 343/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7948 - accuracy: 0.7361 - val_loss: 1.5407 - val_accuracy: 0.4171\n",
      "Epoch 344/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7956 - accuracy: 0.7374 - val_loss: 1.5359 - val_accuracy: 0.3957\n",
      "Epoch 345/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7893 - accuracy: 0.7141 - val_loss: 1.5729 - val_accuracy: 0.3957\n",
      "Epoch 346/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7801 - accuracy: 0.7426 - val_loss: 1.5132 - val_accuracy: 0.4332\n",
      "Epoch 347/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7848 - accuracy: 0.7387 - val_loss: 1.5502 - val_accuracy: 0.4064\n",
      "Epoch 348/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7872 - accuracy: 0.7335 - val_loss: 1.5308 - val_accuracy: 0.4118\n",
      "Epoch 349/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7801 - accuracy: 0.7374 - val_loss: 1.5541 - val_accuracy: 0.4064\n",
      "Epoch 350/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7869 - accuracy: 0.7296 - val_loss: 1.5803 - val_accuracy: 0.4171\n",
      "Epoch 351/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7711 - accuracy: 0.7451 - val_loss: 1.6086 - val_accuracy: 0.4278\n",
      "Epoch 352/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7778 - accuracy: 0.7257 - val_loss: 1.5859 - val_accuracy: 0.4011\n",
      "Epoch 353/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7677 - accuracy: 0.7283 - val_loss: 1.5700 - val_accuracy: 0.4171\n",
      "Epoch 354/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7808 - accuracy: 0.7270 - val_loss: 1.5532 - val_accuracy: 0.4278\n",
      "Epoch 355/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7764 - accuracy: 0.7335 - val_loss: 1.5395 - val_accuracy: 0.4385\n",
      "Epoch 356/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7682 - accuracy: 0.7400 - val_loss: 1.5382 - val_accuracy: 0.4278\n",
      "Epoch 357/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7633 - accuracy: 0.7413 - val_loss: 1.5211 - val_accuracy: 0.4332\n",
      "Epoch 358/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.7639 - accuracy: 0.7439 - val_loss: 1.5383 - val_accuracy: 0.4064\n",
      "Epoch 359/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7635 - accuracy: 0.7348 - val_loss: 1.5850 - val_accuracy: 0.4225\n",
      "Epoch 360/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.7632 - accuracy: 0.7464 - val_loss: 1.5876 - val_accuracy: 0.4492\n",
      "Epoch 361/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.7633 - accuracy: 0.7400 - val_loss: 1.6157 - val_accuracy: 0.3904\n",
      "Epoch 362/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7561 - accuracy: 0.7503 - val_loss: 1.5829 - val_accuracy: 0.4385\n",
      "Epoch 363/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7621 - accuracy: 0.7270 - val_loss: 1.5415 - val_accuracy: 0.4118\n",
      "Epoch 364/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7449 - accuracy: 0.7439 - val_loss: 1.5801 - val_accuracy: 0.4439\n",
      "Epoch 365/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7463 - accuracy: 0.7400 - val_loss: 1.5580 - val_accuracy: 0.4225\n",
      "Epoch 366/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7407 - accuracy: 0.7555 - val_loss: 1.5415 - val_accuracy: 0.4332\n",
      "Epoch 367/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7510 - accuracy: 0.7322 - val_loss: 1.5640 - val_accuracy: 0.4171\n",
      "Epoch 368/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7483 - accuracy: 0.7542 - val_loss: 1.6786 - val_accuracy: 0.3690\n",
      "Epoch 369/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7542 - accuracy: 0.7490 - val_loss: 1.5509 - val_accuracy: 0.4225\n",
      "Epoch 370/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7500 - accuracy: 0.7503 - val_loss: 1.5707 - val_accuracy: 0.4225\n",
      "Epoch 371/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7451 - accuracy: 0.7477 - val_loss: 1.5631 - val_accuracy: 0.4011\n",
      "Epoch 372/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.7361 - accuracy: 0.7503 - val_loss: 1.5726 - val_accuracy: 0.4332\n",
      "Epoch 373/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7320 - accuracy: 0.7516 - val_loss: 1.5715 - val_accuracy: 0.4118\n",
      "Epoch 374/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7341 - accuracy: 0.7542 - val_loss: 1.6079 - val_accuracy: 0.4064\n",
      "Epoch 375/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7335 - accuracy: 0.7568 - val_loss: 1.6172 - val_accuracy: 0.4011\n",
      "Epoch 376/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7391 - accuracy: 0.7439 - val_loss: 1.5734 - val_accuracy: 0.4064\n",
      "Epoch 377/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7299 - accuracy: 0.7555 - val_loss: 1.5880 - val_accuracy: 0.4278\n",
      "Epoch 378/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7368 - accuracy: 0.7387 - val_loss: 1.6218 - val_accuracy: 0.3957\n",
      "Epoch 379/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7222 - accuracy: 0.7542 - val_loss: 1.5636 - val_accuracy: 0.4225\n",
      "Epoch 380/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7294 - accuracy: 0.7503 - val_loss: 1.5438 - val_accuracy: 0.4278\n",
      "Epoch 381/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7302 - accuracy: 0.7646 - val_loss: 1.5324 - val_accuracy: 0.4171\n",
      "Epoch 382/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7227 - accuracy: 0.7607 - val_loss: 1.5700 - val_accuracy: 0.4332\n",
      "Epoch 383/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7202 - accuracy: 0.7568 - val_loss: 1.6500 - val_accuracy: 0.4385\n",
      "Epoch 384/700\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.7247 - accuracy: 0.7490 - val_loss: 1.6089 - val_accuracy: 0.3957\n",
      "Epoch 385/700\n",
      "773/773 [==============================] - 6s 7ms/step - loss: 0.7163 - accuracy: 0.7646 - val_loss: 1.5819 - val_accuracy: 0.4332\n",
      "Epoch 386/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.7124 - accuracy: 0.7620 - val_loss: 1.5976 - val_accuracy: 0.3957\n",
      "Epoch 387/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.7164 - accuracy: 0.7658 - val_loss: 1.6221 - val_accuracy: 0.4385\n",
      "Epoch 388/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.7064 - accuracy: 0.7581 - val_loss: 1.6136 - val_accuracy: 0.4225\n",
      "Epoch 389/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.7044 - accuracy: 0.7568 - val_loss: 1.5609 - val_accuracy: 0.4225\n",
      "Epoch 390/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.7096 - accuracy: 0.7710 - val_loss: 1.5499 - val_accuracy: 0.4171\n",
      "Epoch 391/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.7048 - accuracy: 0.7684 - val_loss: 1.5582 - val_accuracy: 0.4064\n",
      "Epoch 392/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.7062 - accuracy: 0.7671 - val_loss: 1.5657 - val_accuracy: 0.3904\n",
      "Epoch 393/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/773 [==============================] - 6s 8ms/step - loss: 0.7039 - accuracy: 0.7736 - val_loss: 1.6057 - val_accuracy: 0.4011\n",
      "Epoch 394/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.7059 - accuracy: 0.7646 - val_loss: 1.5710 - val_accuracy: 0.4439\n",
      "Epoch 395/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6905 - accuracy: 0.7646 - val_loss: 1.5641 - val_accuracy: 0.4118\n",
      "Epoch 396/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6974 - accuracy: 0.7646 - val_loss: 1.5883 - val_accuracy: 0.4385\n",
      "Epoch 397/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.7697 - val_loss: 1.5806 - val_accuracy: 0.4171\n",
      "Epoch 398/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6826 - accuracy: 0.7749 - val_loss: 1.5709 - val_accuracy: 0.4545\n",
      "Epoch 399/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6969 - accuracy: 0.7723 - val_loss: 1.5999 - val_accuracy: 0.4332\n",
      "Epoch 400/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6890 - accuracy: 0.7697 - val_loss: 1.5853 - val_accuracy: 0.4171\n",
      "Epoch 401/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6957 - accuracy: 0.7646 - val_loss: 1.5996 - val_accuracy: 0.3904\n",
      "Epoch 402/700\n",
      "773/773 [==============================] - 7s 8ms/step - loss: 0.6812 - accuracy: 0.7827 - val_loss: 1.6006 - val_accuracy: 0.4225\n",
      "Epoch 403/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.6865 - accuracy: 0.7827 - val_loss: 1.6081 - val_accuracy: 0.3957\n",
      "Epoch 404/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.6825 - accuracy: 0.7684 - val_loss: 1.5842 - val_accuracy: 0.4118\n",
      "Epoch 405/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6900 - accuracy: 0.7801 - val_loss: 1.5921 - val_accuracy: 0.3904\n",
      "Epoch 406/700\n",
      "773/773 [==============================] - 7s 8ms/step - loss: 0.6811 - accuracy: 0.7633 - val_loss: 1.5878 - val_accuracy: 0.4385\n",
      "Epoch 407/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6828 - accuracy: 0.7723 - val_loss: 1.6665 - val_accuracy: 0.4118\n",
      "Epoch 408/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6764 - accuracy: 0.7788 - val_loss: 1.6027 - val_accuracy: 0.3904\n",
      "Epoch 409/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6728 - accuracy: 0.7801 - val_loss: 1.6312 - val_accuracy: 0.4225\n",
      "Epoch 410/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6777 - accuracy: 0.7736 - val_loss: 1.6425 - val_accuracy: 0.4064\n",
      "Epoch 411/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6662 - accuracy: 0.7891 - val_loss: 1.5995 - val_accuracy: 0.4011\n",
      "Epoch 412/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.6654 - accuracy: 0.7891 - val_loss: 1.5752 - val_accuracy: 0.4064\n",
      "Epoch 413/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6653 - accuracy: 0.7853 - val_loss: 1.6045 - val_accuracy: 0.4011\n",
      "Epoch 414/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6715 - accuracy: 0.7658 - val_loss: 1.6032 - val_accuracy: 0.4064\n",
      "Epoch 415/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6622 - accuracy: 0.7801 - val_loss: 1.6702 - val_accuracy: 0.4064\n",
      "Epoch 416/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6660 - accuracy: 0.7853 - val_loss: 1.6132 - val_accuracy: 0.4064\n",
      "Epoch 417/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6666 - accuracy: 0.7749 - val_loss: 1.5694 - val_accuracy: 0.4225\n",
      "Epoch 418/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6511 - accuracy: 0.7840 - val_loss: 1.6294 - val_accuracy: 0.4011\n",
      "Epoch 419/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6546 - accuracy: 0.7853 - val_loss: 1.5960 - val_accuracy: 0.4278\n",
      "Epoch 420/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6548 - accuracy: 0.7840 - val_loss: 1.6487 - val_accuracy: 0.3957\n",
      "Epoch 421/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6489 - accuracy: 0.7969 - val_loss: 1.6168 - val_accuracy: 0.3957\n",
      "Epoch 422/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6535 - accuracy: 0.7827 - val_loss: 1.6261 - val_accuracy: 0.4118\n",
      "Epoch 423/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6525 - accuracy: 0.7865 - val_loss: 1.6586 - val_accuracy: 0.4332\n",
      "Epoch 424/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6529 - accuracy: 0.7697 - val_loss: 1.5971 - val_accuracy: 0.4118\n",
      "Epoch 425/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.7917 - val_loss: 1.6077 - val_accuracy: 0.3957\n",
      "Epoch 426/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6314 - accuracy: 0.7917 - val_loss: 1.5962 - val_accuracy: 0.4118\n",
      "Epoch 427/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6344 - accuracy: 0.8150 - val_loss: 1.6202 - val_accuracy: 0.4439\n",
      "Epoch 428/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6435 - accuracy: 0.7917 - val_loss: 1.6107 - val_accuracy: 0.4332\n",
      "Epoch 429/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6391 - accuracy: 0.7943 - val_loss: 1.6162 - val_accuracy: 0.4278\n",
      "Epoch 430/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6336 - accuracy: 0.8072 - val_loss: 1.6278 - val_accuracy: 0.4225\n",
      "Epoch 431/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6314 - accuracy: 0.7865 - val_loss: 1.7457 - val_accuracy: 0.3904\n",
      "Epoch 432/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6345 - accuracy: 0.7865 - val_loss: 1.6269 - val_accuracy: 0.4385\n",
      "Epoch 433/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6293 - accuracy: 0.7891 - val_loss: 1.6199 - val_accuracy: 0.3904\n",
      "Epoch 434/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6247 - accuracy: 0.7749 - val_loss: 1.6026 - val_accuracy: 0.4225\n",
      "Epoch 435/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6203 - accuracy: 0.8034 - val_loss: 1.6612 - val_accuracy: 0.3904\n",
      "Epoch 436/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6276 - accuracy: 0.7840 - val_loss: 1.6578 - val_accuracy: 0.4011\n",
      "Epoch 437/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6294 - accuracy: 0.7982 - val_loss: 1.6048 - val_accuracy: 0.4011\n",
      "Epoch 438/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6222 - accuracy: 0.7969 - val_loss: 1.6120 - val_accuracy: 0.4064\n",
      "Epoch 439/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.6220 - accuracy: 0.7943 - val_loss: 1.6032 - val_accuracy: 0.4118\n",
      "Epoch 440/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.6209 - accuracy: 0.8034 - val_loss: 1.6067 - val_accuracy: 0.4118\n",
      "Epoch 441/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6167 - accuracy: 0.8008 - val_loss: 1.6685 - val_accuracy: 0.4064\n",
      "Epoch 442/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6130 - accuracy: 0.8034 - val_loss: 1.6947 - val_accuracy: 0.3850\n",
      "Epoch 443/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6132 - accuracy: 0.7982 - val_loss: 1.6597 - val_accuracy: 0.3904\n",
      "Epoch 444/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6056 - accuracy: 0.8008 - val_loss: 1.6703 - val_accuracy: 0.4011\n",
      "Epoch 445/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6074 - accuracy: 0.8085 - val_loss: 1.6160 - val_accuracy: 0.3957\n",
      "Epoch 446/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6104 - accuracy: 0.7995 - val_loss: 1.5991 - val_accuracy: 0.4064\n",
      "Epoch 447/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6077 - accuracy: 0.8034 - val_loss: 1.6476 - val_accuracy: 0.4332\n",
      "Epoch 448/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6047 - accuracy: 0.8060 - val_loss: 1.6244 - val_accuracy: 0.4118\n",
      "Epoch 449/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/773 [==============================] - 6s 8ms/step - loss: 0.6086 - accuracy: 0.7982 - val_loss: 1.6724 - val_accuracy: 0.3850\n",
      "Epoch 450/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5971 - accuracy: 0.8137 - val_loss: 1.6847 - val_accuracy: 0.4118\n",
      "Epoch 451/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5974 - accuracy: 0.8047 - val_loss: 1.6659 - val_accuracy: 0.3850\n",
      "Epoch 452/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5986 - accuracy: 0.8150 - val_loss: 1.6232 - val_accuracy: 0.4278\n",
      "Epoch 453/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5978 - accuracy: 0.8047 - val_loss: 1.6409 - val_accuracy: 0.4171\n",
      "Epoch 454/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5937 - accuracy: 0.8034 - val_loss: 1.6664 - val_accuracy: 0.4171\n",
      "Epoch 455/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5904 - accuracy: 0.8163 - val_loss: 1.6514 - val_accuracy: 0.4332\n",
      "Epoch 456/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5806 - accuracy: 0.8241 - val_loss: 1.6385 - val_accuracy: 0.4011\n",
      "Epoch 457/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5883 - accuracy: 0.8098 - val_loss: 1.6847 - val_accuracy: 0.3904\n",
      "Epoch 458/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5814 - accuracy: 0.8176 - val_loss: 1.7264 - val_accuracy: 0.4225\n",
      "Epoch 459/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5800 - accuracy: 0.8047 - val_loss: 1.6358 - val_accuracy: 0.4011\n",
      "Epoch 460/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5877 - accuracy: 0.8137 - val_loss: 1.6413 - val_accuracy: 0.4225\n",
      "Epoch 461/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5773 - accuracy: 0.8137 - val_loss: 1.6674 - val_accuracy: 0.4011\n",
      "Epoch 462/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5917 - accuracy: 0.8124 - val_loss: 1.6257 - val_accuracy: 0.4064\n",
      "Epoch 463/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5729 - accuracy: 0.8202 - val_loss: 1.6427 - val_accuracy: 0.3904\n",
      "Epoch 464/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5714 - accuracy: 0.8241 - val_loss: 1.6870 - val_accuracy: 0.3797\n",
      "Epoch 465/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5719 - accuracy: 0.8085 - val_loss: 1.6515 - val_accuracy: 0.4439\n",
      "Epoch 466/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5799 - accuracy: 0.8124 - val_loss: 1.6612 - val_accuracy: 0.4225\n",
      "Epoch 467/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5739 - accuracy: 0.8266 - val_loss: 1.6464 - val_accuracy: 0.4225\n",
      "Epoch 468/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5656 - accuracy: 0.8111 - val_loss: 1.6340 - val_accuracy: 0.4064\n",
      "Epoch 469/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5635 - accuracy: 0.8215 - val_loss: 1.6830 - val_accuracy: 0.4171\n",
      "Epoch 470/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5640 - accuracy: 0.8266 - val_loss: 1.6459 - val_accuracy: 0.4225\n",
      "Epoch 471/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5556 - accuracy: 0.8215 - val_loss: 1.6549 - val_accuracy: 0.4385\n",
      "Epoch 472/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5597 - accuracy: 0.8318 - val_loss: 1.6755 - val_accuracy: 0.4064\n",
      "Epoch 473/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5578 - accuracy: 0.8176 - val_loss: 1.7098 - val_accuracy: 0.3743\n",
      "Epoch 474/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5531 - accuracy: 0.8357 - val_loss: 1.7028 - val_accuracy: 0.4011\n",
      "Epoch 475/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5543 - accuracy: 0.8357 - val_loss: 1.7157 - val_accuracy: 0.3904\n",
      "Epoch 476/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5495 - accuracy: 0.8292 - val_loss: 1.6645 - val_accuracy: 0.4385\n",
      "Epoch 477/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5551 - accuracy: 0.8228 - val_loss: 1.6919 - val_accuracy: 0.4064\n",
      "Epoch 478/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5463 - accuracy: 0.8344 - val_loss: 1.6657 - val_accuracy: 0.4225\n",
      "Epoch 479/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5475 - accuracy: 0.8241 - val_loss: 1.6383 - val_accuracy: 0.4332\n",
      "Epoch 480/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5508 - accuracy: 0.8344 - val_loss: 1.6589 - val_accuracy: 0.4064\n",
      "Epoch 481/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5387 - accuracy: 0.8279 - val_loss: 1.7018 - val_accuracy: 0.4171\n",
      "Epoch 482/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5482 - accuracy: 0.8292 - val_loss: 1.6801 - val_accuracy: 0.4064\n",
      "Epoch 483/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5454 - accuracy: 0.8266 - val_loss: 1.6543 - val_accuracy: 0.4064\n",
      "Epoch 484/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5416 - accuracy: 0.8189 - val_loss: 1.8067 - val_accuracy: 0.3957\n",
      "Epoch 485/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5388 - accuracy: 0.8331 - val_loss: 1.6620 - val_accuracy: 0.4011\n",
      "Epoch 486/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5343 - accuracy: 0.8383 - val_loss: 1.7125 - val_accuracy: 0.4011\n",
      "Epoch 487/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5370 - accuracy: 0.8305 - val_loss: 1.6790 - val_accuracy: 0.4171\n",
      "Epoch 488/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5288 - accuracy: 0.8383 - val_loss: 1.6859 - val_accuracy: 0.4171\n",
      "Epoch 489/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5283 - accuracy: 0.8422 - val_loss: 1.7371 - val_accuracy: 0.3904\n",
      "Epoch 490/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5223 - accuracy: 0.8383 - val_loss: 1.7197 - val_accuracy: 0.4011\n",
      "Epoch 491/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5232 - accuracy: 0.8383 - val_loss: 1.7287 - val_accuracy: 0.4064\n",
      "Epoch 492/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5275 - accuracy: 0.8357 - val_loss: 1.7200 - val_accuracy: 0.3957\n",
      "Epoch 493/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5217 - accuracy: 0.8422 - val_loss: 1.6847 - val_accuracy: 0.4064\n",
      "Epoch 494/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5255 - accuracy: 0.8409 - val_loss: 1.6890 - val_accuracy: 0.3957\n",
      "Epoch 495/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5213 - accuracy: 0.8499 - val_loss: 1.7065 - val_accuracy: 0.4064\n",
      "Epoch 496/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5118 - accuracy: 0.8461 - val_loss: 1.7142 - val_accuracy: 0.4278\n",
      "Epoch 497/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5209 - accuracy: 0.8331 - val_loss: 1.7576 - val_accuracy: 0.4011\n",
      "Epoch 498/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5194 - accuracy: 0.8370 - val_loss: 1.7115 - val_accuracy: 0.4225\n",
      "Epoch 499/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5096 - accuracy: 0.8577 - val_loss: 1.6779 - val_accuracy: 0.3957\n",
      "Epoch 500/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5091 - accuracy: 0.8344 - val_loss: 1.7305 - val_accuracy: 0.4225\n",
      "Epoch 501/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5167 - accuracy: 0.8422 - val_loss: 1.7260 - val_accuracy: 0.4064\n",
      "Epoch 502/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5034 - accuracy: 0.8512 - val_loss: 1.6904 - val_accuracy: 0.3957\n",
      "Epoch 503/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4957 - accuracy: 0.8693 - val_loss: 1.7285 - val_accuracy: 0.4064\n",
      "Epoch 504/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5001 - accuracy: 0.8551 - val_loss: 1.7123 - val_accuracy: 0.4225\n",
      "Epoch 505/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5065 - accuracy: 0.8525 - val_loss: 1.6846 - val_accuracy: 0.4011\n",
      "Epoch 506/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4977 - accuracy: 0.8525 - val_loss: 1.6902 - val_accuracy: 0.4064\n",
      "Epoch 507/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4985 - accuracy: 0.8590 - val_loss: 1.7102 - val_accuracy: 0.4278\n",
      "Epoch 508/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4917 - accuracy: 0.8473 - val_loss: 1.7302 - val_accuracy: 0.4011\n",
      "Epoch 509/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4870 - accuracy: 0.8499 - val_loss: 1.7094 - val_accuracy: 0.3957\n",
      "Epoch 510/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.5000 - accuracy: 0.8461 - val_loss: 1.7160 - val_accuracy: 0.3957\n",
      "Epoch 511/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4869 - accuracy: 0.8525 - val_loss: 1.7601 - val_accuracy: 0.4064\n",
      "Epoch 512/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4908 - accuracy: 0.8525 - val_loss: 1.7053 - val_accuracy: 0.4118\n",
      "Epoch 513/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4762 - accuracy: 0.8564 - val_loss: 1.8114 - val_accuracy: 0.3636\n",
      "Epoch 514/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4808 - accuracy: 0.8538 - val_loss: 1.7225 - val_accuracy: 0.4011\n",
      "Epoch 515/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4861 - accuracy: 0.8551 - val_loss: 1.7592 - val_accuracy: 0.4011\n",
      "Epoch 516/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4718 - accuracy: 0.8680 - val_loss: 1.8520 - val_accuracy: 0.4118\n",
      "Epoch 517/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4853 - accuracy: 0.8461 - val_loss: 1.7245 - val_accuracy: 0.3850\n",
      "Epoch 518/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4825 - accuracy: 0.8564 - val_loss: 1.7727 - val_accuracy: 0.4011\n",
      "Epoch 519/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4688 - accuracy: 0.8655 - val_loss: 1.7441 - val_accuracy: 0.3690\n",
      "Epoch 520/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4725 - accuracy: 0.8616 - val_loss: 1.7400 - val_accuracy: 0.4064\n",
      "Epoch 521/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4752 - accuracy: 0.8642 - val_loss: 1.7478 - val_accuracy: 0.4064\n",
      "Epoch 522/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4741 - accuracy: 0.8668 - val_loss: 1.7120 - val_accuracy: 0.4011\n",
      "Epoch 523/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4705 - accuracy: 0.8655 - val_loss: 1.7340 - val_accuracy: 0.3904\n",
      "Epoch 524/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4670 - accuracy: 0.8668 - val_loss: 1.7738 - val_accuracy: 0.3957\n",
      "Epoch 525/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4673 - accuracy: 0.8603 - val_loss: 1.7463 - val_accuracy: 0.4064\n",
      "Epoch 526/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4700 - accuracy: 0.8525 - val_loss: 1.7263 - val_accuracy: 0.4278\n",
      "Epoch 527/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4601 - accuracy: 0.8719 - val_loss: 1.7428 - val_accuracy: 0.4118\n",
      "Epoch 528/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4598 - accuracy: 0.8616 - val_loss: 1.7309 - val_accuracy: 0.3957\n",
      "Epoch 529/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4574 - accuracy: 0.8668 - val_loss: 1.7515 - val_accuracy: 0.4118\n",
      "Epoch 530/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4536 - accuracy: 0.8745 - val_loss: 1.7874 - val_accuracy: 0.3957\n",
      "Epoch 531/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4554 - accuracy: 0.8758 - val_loss: 1.7683 - val_accuracy: 0.4278\n",
      "Epoch 532/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4500 - accuracy: 0.8771 - val_loss: 1.7358 - val_accuracy: 0.4118\n",
      "Epoch 533/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4474 - accuracy: 0.8771 - val_loss: 1.7510 - val_accuracy: 0.4118\n",
      "Epoch 534/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4470 - accuracy: 0.8836 - val_loss: 1.7475 - val_accuracy: 0.4011\n",
      "Epoch 535/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4464 - accuracy: 0.8758 - val_loss: 1.7423 - val_accuracy: 0.4118\n",
      "Epoch 536/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4454 - accuracy: 0.8590 - val_loss: 1.7693 - val_accuracy: 0.4118\n",
      "Epoch 537/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4505 - accuracy: 0.8655 - val_loss: 1.7687 - val_accuracy: 0.4064\n",
      "Epoch 538/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4451 - accuracy: 0.8719 - val_loss: 1.7684 - val_accuracy: 0.4118\n",
      "Epoch 539/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4414 - accuracy: 0.8668 - val_loss: 1.8451 - val_accuracy: 0.3957\n",
      "Epoch 540/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4338 - accuracy: 0.8706 - val_loss: 1.7638 - val_accuracy: 0.4011\n",
      "Epoch 541/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4353 - accuracy: 0.8680 - val_loss: 1.7738 - val_accuracy: 0.4118\n",
      "Epoch 542/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4406 - accuracy: 0.8758 - val_loss: 1.7780 - val_accuracy: 0.4064\n",
      "Epoch 543/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4246 - accuracy: 0.8836 - val_loss: 1.8380 - val_accuracy: 0.4171\n",
      "Epoch 544/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4285 - accuracy: 0.8797 - val_loss: 1.7932 - val_accuracy: 0.3904\n",
      "Epoch 545/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4229 - accuracy: 0.8797 - val_loss: 1.8392 - val_accuracy: 0.3904\n",
      "Epoch 546/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4282 - accuracy: 0.8693 - val_loss: 1.8279 - val_accuracy: 0.4011\n",
      "Epoch 547/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4283 - accuracy: 0.8836 - val_loss: 1.8466 - val_accuracy: 0.3904\n",
      "Epoch 548/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4262 - accuracy: 0.8836 - val_loss: 1.8204 - val_accuracy: 0.4011\n",
      "Epoch 549/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4230 - accuracy: 0.8849 - val_loss: 1.8356 - val_accuracy: 0.3957\n",
      "Epoch 550/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4257 - accuracy: 0.8862 - val_loss: 1.8050 - val_accuracy: 0.4118\n",
      "Epoch 551/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4126 - accuracy: 0.8836 - val_loss: 1.7839 - val_accuracy: 0.4011\n",
      "Epoch 552/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4221 - accuracy: 0.8784 - val_loss: 1.7957 - val_accuracy: 0.4011\n",
      "Epoch 553/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4111 - accuracy: 0.8836 - val_loss: 1.7770 - val_accuracy: 0.4278\n",
      "Epoch 554/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4200 - accuracy: 0.8939 - val_loss: 1.7854 - val_accuracy: 0.4064\n",
      "Epoch 555/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4132 - accuracy: 0.8887 - val_loss: 1.7998 - val_accuracy: 0.4011\n",
      "Epoch 556/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4073 - accuracy: 0.8939 - val_loss: 1.7888 - val_accuracy: 0.4225\n",
      "Epoch 557/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4113 - accuracy: 0.8926 - val_loss: 1.7857 - val_accuracy: 0.4064\n",
      "Epoch 558/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4094 - accuracy: 0.8849 - val_loss: 1.7967 - val_accuracy: 0.4278\n",
      "Epoch 559/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4094 - accuracy: 0.8900 - val_loss: 1.8389 - val_accuracy: 0.4118\n",
      "Epoch 560/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4081 - accuracy: 0.8952 - val_loss: 1.8414 - val_accuracy: 0.3850\n",
      "Epoch 561/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4075 - accuracy: 0.8887 - val_loss: 1.8288 - val_accuracy: 0.4118\n",
      "Epoch 562/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4039 - accuracy: 0.8926 - val_loss: 1.8058 - val_accuracy: 0.4118\n",
      "Epoch 563/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3956 - accuracy: 0.8913 - val_loss: 1.8079 - val_accuracy: 0.4118\n",
      "Epoch 564/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4079 - accuracy: 0.8823 - val_loss: 1.8152 - val_accuracy: 0.3957\n",
      "Epoch 565/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3966 - accuracy: 0.8849 - val_loss: 1.8038 - val_accuracy: 0.4225\n",
      "Epoch 566/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3981 - accuracy: 0.8823 - val_loss: 1.8100 - val_accuracy: 0.4118\n",
      "Epoch 567/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3928 - accuracy: 0.8991 - val_loss: 1.8144 - val_accuracy: 0.4064\n",
      "Epoch 568/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3900 - accuracy: 0.9017 - val_loss: 1.8378 - val_accuracy: 0.4171\n",
      "Epoch 569/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.4024 - accuracy: 0.8913 - val_loss: 1.8021 - val_accuracy: 0.4332\n",
      "Epoch 570/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3871 - accuracy: 0.9043 - val_loss: 1.8151 - val_accuracy: 0.3850\n",
      "Epoch 571/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3883 - accuracy: 0.8849 - val_loss: 1.7998 - val_accuracy: 0.4011\n",
      "Epoch 572/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3877 - accuracy: 0.8939 - val_loss: 1.8338 - val_accuracy: 0.4118\n",
      "Epoch 573/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3750 - accuracy: 0.9133 - val_loss: 1.9214 - val_accuracy: 0.3797\n",
      "Epoch 574/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3872 - accuracy: 0.8991 - val_loss: 1.8033 - val_accuracy: 0.4171\n",
      "Epoch 575/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3803 - accuracy: 0.8875 - val_loss: 1.8792 - val_accuracy: 0.3904\n",
      "Epoch 576/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3752 - accuracy: 0.8952 - val_loss: 1.9374 - val_accuracy: 0.4118\n",
      "Epoch 577/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3820 - accuracy: 0.9017 - val_loss: 1.8515 - val_accuracy: 0.4118\n",
      "Epoch 578/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3764 - accuracy: 0.8978 - val_loss: 1.8421 - val_accuracy: 0.4118\n",
      "Epoch 579/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3701 - accuracy: 0.8965 - val_loss: 1.8475 - val_accuracy: 0.4011\n",
      "Epoch 580/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3731 - accuracy: 0.8965 - val_loss: 1.8445 - val_accuracy: 0.4171\n",
      "Epoch 581/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3715 - accuracy: 0.8939 - val_loss: 1.8238 - val_accuracy: 0.4278\n",
      "Epoch 582/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3723 - accuracy: 0.9082 - val_loss: 1.9036 - val_accuracy: 0.4064\n",
      "Epoch 583/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3739 - accuracy: 0.9069 - val_loss: 1.8348 - val_accuracy: 0.3850\n",
      "Epoch 584/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3715 - accuracy: 0.9133 - val_loss: 1.8468 - val_accuracy: 0.4118\n",
      "Epoch 585/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3642 - accuracy: 0.9030 - val_loss: 1.8486 - val_accuracy: 0.4064\n",
      "Epoch 586/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3567 - accuracy: 0.9056 - val_loss: 1.8634 - val_accuracy: 0.4118\n",
      "Epoch 587/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3559 - accuracy: 0.9146 - val_loss: 1.8835 - val_accuracy: 0.4278\n",
      "Epoch 588/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3578 - accuracy: 0.9043 - val_loss: 1.8859 - val_accuracy: 0.4011\n",
      "Epoch 589/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3644 - accuracy: 0.9004 - val_loss: 1.8590 - val_accuracy: 0.4225\n",
      "Epoch 590/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3579 - accuracy: 0.9172 - val_loss: 1.8276 - val_accuracy: 0.4278\n",
      "Epoch 591/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3539 - accuracy: 0.9250 - val_loss: 1.8703 - val_accuracy: 0.3850\n",
      "Epoch 592/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3522 - accuracy: 0.9133 - val_loss: 1.8907 - val_accuracy: 0.3904\n",
      "Epoch 593/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3553 - accuracy: 0.9120 - val_loss: 1.9415 - val_accuracy: 0.3743\n",
      "Epoch 594/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3463 - accuracy: 0.9224 - val_loss: 1.9336 - val_accuracy: 0.3850\n",
      "Epoch 595/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3549 - accuracy: 0.9172 - val_loss: 1.9171 - val_accuracy: 0.4118\n",
      "Epoch 596/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3461 - accuracy: 0.9043 - val_loss: 1.9232 - val_accuracy: 0.3957\n",
      "Epoch 597/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3414 - accuracy: 0.9133 - val_loss: 1.9153 - val_accuracy: 0.3850\n",
      "Epoch 598/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3381 - accuracy: 0.9146 - val_loss: 1.8722 - val_accuracy: 0.3850\n",
      "Epoch 599/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3407 - accuracy: 0.9146 - val_loss: 1.9114 - val_accuracy: 0.3797\n",
      "Epoch 600/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3393 - accuracy: 0.9107 - val_loss: 1.8774 - val_accuracy: 0.4171\n",
      "Epoch 601/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3334 - accuracy: 0.9250 - val_loss: 1.9447 - val_accuracy: 0.3850\n",
      "Epoch 602/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3329 - accuracy: 0.9198 - val_loss: 1.9048 - val_accuracy: 0.4118\n",
      "Epoch 603/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3278 - accuracy: 0.9146 - val_loss: 1.9070 - val_accuracy: 0.4171\n",
      "Epoch 604/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3339 - accuracy: 0.9146 - val_loss: 1.8726 - val_accuracy: 0.4225\n",
      "Epoch 605/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3281 - accuracy: 0.9133 - val_loss: 1.9186 - val_accuracy: 0.4332\n",
      "Epoch 606/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.3257 - accuracy: 0.9237 - val_loss: 1.9018 - val_accuracy: 0.4171\n",
      "Epoch 607/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3317 - accuracy: 0.9172 - val_loss: 1.9127 - val_accuracy: 0.4064\n",
      "Epoch 608/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3307 - accuracy: 0.9172 - val_loss: 1.9136 - val_accuracy: 0.4118\n",
      "Epoch 609/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3227 - accuracy: 0.9276 - val_loss: 1.9024 - val_accuracy: 0.3850\n",
      "Epoch 610/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3244 - accuracy: 0.9198 - val_loss: 1.8817 - val_accuracy: 0.4064\n",
      "Epoch 611/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3146 - accuracy: 0.9224 - val_loss: 1.9271 - val_accuracy: 0.4171\n",
      "Epoch 612/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3151 - accuracy: 0.9340 - val_loss: 1.9272 - val_accuracy: 0.4225\n",
      "Epoch 613/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3178 - accuracy: 0.9263 - val_loss: 1.8845 - val_accuracy: 0.4171\n",
      "Epoch 614/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3189 - accuracy: 0.9276 - val_loss: 1.9899 - val_accuracy: 0.3904\n",
      "Epoch 615/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3133 - accuracy: 0.9327 - val_loss: 1.9876 - val_accuracy: 0.4064\n",
      "Epoch 616/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3049 - accuracy: 0.9250 - val_loss: 1.9140 - val_accuracy: 0.3957\n",
      "Epoch 617/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3126 - accuracy: 0.9198 - val_loss: 1.9412 - val_accuracy: 0.4011\n",
      "Epoch 618/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3093 - accuracy: 0.9301 - val_loss: 1.9629 - val_accuracy: 0.4064\n",
      "Epoch 619/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3020 - accuracy: 0.9314 - val_loss: 1.9152 - val_accuracy: 0.3904\n",
      "Epoch 620/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3104 - accuracy: 0.9224 - val_loss: 1.9392 - val_accuracy: 0.3957\n",
      "Epoch 621/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2975 - accuracy: 0.9288 - val_loss: 1.9674 - val_accuracy: 0.3957\n",
      "Epoch 622/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2977 - accuracy: 0.9301 - val_loss: 1.9771 - val_accuracy: 0.4011\n",
      "Epoch 623/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3027 - accuracy: 0.9288 - val_loss: 1.9543 - val_accuracy: 0.3904\n",
      "Epoch 624/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3034 - accuracy: 0.9340 - val_loss: 1.9468 - val_accuracy: 0.3957\n",
      "Epoch 625/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2966 - accuracy: 0.9340 - val_loss: 1.9847 - val_accuracy: 0.3957\n",
      "Epoch 626/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2992 - accuracy: 0.9276 - val_loss: 1.9841 - val_accuracy: 0.3904\n",
      "Epoch 627/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2865 - accuracy: 0.9431 - val_loss: 1.9660 - val_accuracy: 0.4064\n",
      "Epoch 628/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2943 - accuracy: 0.9314 - val_loss: 2.0130 - val_accuracy: 0.4064\n",
      "Epoch 629/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2879 - accuracy: 0.9353 - val_loss: 2.0144 - val_accuracy: 0.3797\n",
      "Epoch 630/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2989 - accuracy: 0.9301 - val_loss: 1.9482 - val_accuracy: 0.3850\n",
      "Epoch 631/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2876 - accuracy: 0.9314 - val_loss: 1.9610 - val_accuracy: 0.4064\n",
      "Epoch 632/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2846 - accuracy: 0.9314 - val_loss: 1.9290 - val_accuracy: 0.4118\n",
      "Epoch 633/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2814 - accuracy: 0.9327 - val_loss: 1.9725 - val_accuracy: 0.4064\n",
      "Epoch 634/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2935 - accuracy: 0.9327 - val_loss: 1.9821 - val_accuracy: 0.4278\n",
      "Epoch 635/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2808 - accuracy: 0.9379 - val_loss: 2.0005 - val_accuracy: 0.4439\n",
      "Epoch 636/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2812 - accuracy: 0.9353 - val_loss: 1.9917 - val_accuracy: 0.3904\n",
      "Epoch 637/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2760 - accuracy: 0.9392 - val_loss: 1.9888 - val_accuracy: 0.3957\n",
      "Epoch 638/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2756 - accuracy: 0.9379 - val_loss: 2.0681 - val_accuracy: 0.4225\n",
      "Epoch 639/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2778 - accuracy: 0.9276 - val_loss: 2.0096 - val_accuracy: 0.4171\n",
      "Epoch 640/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2687 - accuracy: 0.9379 - val_loss: 2.0296 - val_accuracy: 0.4064\n",
      "Epoch 641/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2744 - accuracy: 0.9366 - val_loss: 2.0681 - val_accuracy: 0.4064\n",
      "Epoch 642/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2754 - accuracy: 0.9470 - val_loss: 1.9814 - val_accuracy: 0.3957\n",
      "Epoch 643/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2734 - accuracy: 0.9340 - val_loss: 2.0038 - val_accuracy: 0.4171\n",
      "Epoch 644/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2670 - accuracy: 0.9418 - val_loss: 1.9662 - val_accuracy: 0.4171\n",
      "Epoch 645/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2663 - accuracy: 0.9431 - val_loss: 2.0279 - val_accuracy: 0.3904\n",
      "Epoch 646/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2636 - accuracy: 0.9392 - val_loss: 2.0223 - val_accuracy: 0.4225\n",
      "Epoch 647/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2604 - accuracy: 0.9431 - val_loss: 2.0343 - val_accuracy: 0.4011\n",
      "Epoch 648/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2629 - accuracy: 0.9405 - val_loss: 2.0058 - val_accuracy: 0.3904\n",
      "Epoch 649/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2575 - accuracy: 0.9470 - val_loss: 2.0473 - val_accuracy: 0.4064\n",
      "Epoch 650/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2666 - accuracy: 0.9353 - val_loss: 1.9904 - val_accuracy: 0.4171\n",
      "Epoch 651/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2528 - accuracy: 0.9508 - val_loss: 2.0267 - val_accuracy: 0.3850\n",
      "Epoch 652/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2610 - accuracy: 0.9470 - val_loss: 1.9726 - val_accuracy: 0.4171\n",
      "Epoch 653/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2523 - accuracy: 0.9483 - val_loss: 2.0584 - val_accuracy: 0.4118\n",
      "Epoch 654/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2562 - accuracy: 0.9340 - val_loss: 1.9887 - val_accuracy: 0.4011\n",
      "Epoch 655/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2445 - accuracy: 0.9495 - val_loss: 1.9972 - val_accuracy: 0.3957\n",
      "Epoch 656/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2576 - accuracy: 0.9431 - val_loss: 2.0112 - val_accuracy: 0.4332\n",
      "Epoch 657/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2523 - accuracy: 0.9470 - val_loss: 2.0064 - val_accuracy: 0.4064\n",
      "Epoch 658/700\n",
      "773/773 [==============================] - 7s 9ms/step - loss: 0.2596 - accuracy: 0.9366 - val_loss: 2.0065 - val_accuracy: 0.4225\n",
      "Epoch 659/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2484 - accuracy: 0.9444 - val_loss: 2.0127 - val_accuracy: 0.4118\n",
      "Epoch 660/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2403 - accuracy: 0.9495 - val_loss: 2.0295 - val_accuracy: 0.4064\n",
      "Epoch 661/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2469 - accuracy: 0.9483 - val_loss: 2.0160 - val_accuracy: 0.4225\n",
      "Epoch 662/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2395 - accuracy: 0.9418 - val_loss: 2.0568 - val_accuracy: 0.4225\n",
      "Epoch 663/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2354 - accuracy: 0.9547 - val_loss: 2.1458 - val_accuracy: 0.3797\n",
      "Epoch 664/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2407 - accuracy: 0.9495 - val_loss: 2.1152 - val_accuracy: 0.3583\n",
      "Epoch 665/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2363 - accuracy: 0.9534 - val_loss: 2.0679 - val_accuracy: 0.4064\n",
      "Epoch 666/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2390 - accuracy: 0.9547 - val_loss: 2.0556 - val_accuracy: 0.4225\n",
      "Epoch 667/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2281 - accuracy: 0.9547 - val_loss: 2.0824 - val_accuracy: 0.4652\n",
      "Epoch 668/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2353 - accuracy: 0.9521 - val_loss: 2.1047 - val_accuracy: 0.4011\n",
      "Epoch 669/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2281 - accuracy: 0.9457 - val_loss: 2.0589 - val_accuracy: 0.4278\n",
      "Epoch 670/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2365 - accuracy: 0.9521 - val_loss: 2.0707 - val_accuracy: 0.3850\n",
      "Epoch 671/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2325 - accuracy: 0.9483 - val_loss: 2.0350 - val_accuracy: 0.4118\n",
      "Epoch 672/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2239 - accuracy: 0.9586 - val_loss: 2.0758 - val_accuracy: 0.4118\n",
      "Epoch 673/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2236 - accuracy: 0.9483 - val_loss: 2.1217 - val_accuracy: 0.3850\n",
      "Epoch 674/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2278 - accuracy: 0.9521 - val_loss: 2.0559 - val_accuracy: 0.4064\n",
      "Epoch 675/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2200 - accuracy: 0.9651 - val_loss: 2.0569 - val_accuracy: 0.4385\n",
      "Epoch 676/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2128 - accuracy: 0.9599 - val_loss: 2.1389 - val_accuracy: 0.4118\n",
      "Epoch 677/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2213 - accuracy: 0.9599 - val_loss: 2.0941 - val_accuracy: 0.4011\n",
      "Epoch 678/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2157 - accuracy: 0.9586 - val_loss: 2.0625 - val_accuracy: 0.4011\n",
      "Epoch 679/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2179 - accuracy: 0.9599 - val_loss: 2.1117 - val_accuracy: 0.4171\n",
      "Epoch 680/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2139 - accuracy: 0.9521 - val_loss: 2.0695 - val_accuracy: 0.4064\n",
      "Epoch 681/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2110 - accuracy: 0.9560 - val_loss: 2.0726 - val_accuracy: 0.4225\n",
      "Epoch 682/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2159 - accuracy: 0.9547 - val_loss: 2.0613 - val_accuracy: 0.4118\n",
      "Epoch 683/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2118 - accuracy: 0.9483 - val_loss: 2.0901 - val_accuracy: 0.4064\n",
      "Epoch 684/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2134 - accuracy: 0.9612 - val_loss: 2.1004 - val_accuracy: 0.4064\n",
      "Epoch 685/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2121 - accuracy: 0.9638 - val_loss: 2.0828 - val_accuracy: 0.4011\n",
      "Epoch 686/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2058 - accuracy: 0.9715 - val_loss: 2.0941 - val_accuracy: 0.4011\n",
      "Epoch 687/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2026 - accuracy: 0.9651 - val_loss: 2.1205 - val_accuracy: 0.4171\n",
      "Epoch 688/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2042 - accuracy: 0.9625 - val_loss: 2.0901 - val_accuracy: 0.3850\n",
      "Epoch 689/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2034 - accuracy: 0.9625 - val_loss: 2.0790 - val_accuracy: 0.3797\n",
      "Epoch 690/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2056 - accuracy: 0.9664 - val_loss: 2.1020 - val_accuracy: 0.3797\n",
      "Epoch 691/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.1997 - accuracy: 0.9651 - val_loss: 2.1188 - val_accuracy: 0.4332\n",
      "Epoch 692/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2040 - accuracy: 0.9638 - val_loss: 2.1128 - val_accuracy: 0.3957\n",
      "Epoch 693/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.1992 - accuracy: 0.9638 - val_loss: 2.1713 - val_accuracy: 0.4011\n",
      "Epoch 694/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.2046 - accuracy: 0.9612 - val_loss: 2.1022 - val_accuracy: 0.4278\n",
      "Epoch 695/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.1986 - accuracy: 0.9715 - val_loss: 2.1213 - val_accuracy: 0.4064\n",
      "Epoch 696/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.1908 - accuracy: 0.9651 - val_loss: 2.1074 - val_accuracy: 0.4064\n",
      "Epoch 697/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.1910 - accuracy: 0.9690 - val_loss: 2.1709 - val_accuracy: 0.3743\n",
      "Epoch 698/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.1933 - accuracy: 0.9715 - val_loss: 2.1429 - val_accuracy: 0.3957\n",
      "Epoch 699/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.1904 - accuracy: 0.9599 - val_loss: 2.1719 - val_accuracy: 0.4011\n",
      "Epoch 700/700\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.1922 - accuracy: 0.9625 - val_loss: 2.1555 - val_accuracy: 0.3904\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /home/kanika/Documents/Major/Speech-Emotion-Analyzer-master/saved_models/Emotion_Voice_Detection_Model.h5 \n"
     ]
    }
   ],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "accuracy: 39.04%\n"
     ]
    }
   ],
   "source": [
    "# loading json and creating model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"saved_models/Emotion_Voice_Detection_Model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_testcnn, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting emotions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = loaded_model.predict(x_testcnn, \n",
    "                         batch_size=32, \n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.47428250e-01, 8.22195085e-04, 1.27552636e-03, ...,\n",
       "        2.75677573e-02, 8.34079832e-03, 3.09424549e-02],\n",
       "       [4.91349567e-07, 1.41759005e-08, 1.77573733e-09, ...,\n",
       "        5.94491400e-02, 9.21339452e-01, 9.70969349e-03],\n",
       "       [1.62194192e-03, 2.07708672e-01, 1.91121753e-02, ...,\n",
       "        2.95180289e-05, 3.43761858e-05, 4.09425265e-04],\n",
       "       ...,\n",
       "       [4.02465686e-02, 4.06211824e-04, 1.00519945e-04, ...,\n",
       "        2.23190864e-06, 4.81571669e-06, 3.43493884e-04],\n",
       "       [1.20741269e-03, 2.52288004e-07, 4.09397110e-02, ...,\n",
       "        1.03456026e-04, 9.70715992e-05, 1.26551577e-05],\n",
       "       [1.17035307e-01, 2.80207163e-03, 1.34259768e-04, ...,\n",
       "        5.63142121e-01, 1.67606980e-01, 1.55618764e-04]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1=preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 8, 4, 6, 1, 5, 7, 3, 2, 1, 4, 2, 7, 4, 2, 7, 6, 4, 5, 2, 8, 9,\n",
       "       1, 7, 1, 4, 7, 5, 3, 2, 7, 5, 7, 3, 9, 6, 7, 4, 2, 0, 1, 1, 9, 6,\n",
       "       4, 5, 0, 3, 0, 9, 0, 0, 1, 3, 6, 1, 8, 6, 6, 8, 1, 0, 1, 1, 6, 0,\n",
       "       9, 0, 0, 9, 0, 6, 4, 1, 1, 8, 4, 5, 1, 2, 7, 8, 3, 3, 2, 2, 5, 7,\n",
       "       6, 1, 6, 1, 6, 5, 5, 8, 2, 8, 3, 0, 9, 8, 7, 3, 8, 0, 3, 1, 4, 9,\n",
       "       8, 1, 1, 5, 6, 4, 7, 9, 8, 6, 5, 1, 7, 0, 9, 6, 4, 2, 6, 6, 6, 6,\n",
       "       0, 3, 3, 1, 6, 4, 9, 4, 1, 6, 3, 5, 2, 8, 7, 6, 1, 2, 8, 9, 7, 8,\n",
       "       9, 0, 6, 9, 6, 7, 5, 9, 6, 8, 1, 1, 9, 8, 1, 6, 4, 5, 7, 6, 9, 5,\n",
       "       4, 8, 8, 0, 4, 0, 0, 7, 4, 3, 7])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = preds1.astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (lb.inverse_transform((abc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictedvalues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  predictedvalues\n",
       "0    female_angry\n",
       "1      male_happy\n",
       "2      female_sad\n",
       "3       male_calm\n",
       "4     female_calm\n",
       "5      male_angry\n",
       "6    male_fearful\n",
       "7    female_happy\n",
       "8  female_fearful\n",
       "9     female_calm"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preddf = pd.DataFrame({'predictedvalues': predictions})\n",
    "preddf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual=y_test.argmax(axis=1)\n",
    "abc123 = actual.astype(int).flatten()\n",
    "actualvalues = (lb.inverse_transform((abc123)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actualvalues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     actualvalues\n",
       "0    female_angry\n",
       "1      male_happy\n",
       "2      female_sad\n",
       "3       male_calm\n",
       "4     female_calm\n",
       "5      male_angry\n",
       "6    male_fearful\n",
       "7    male_fearful\n",
       "8  female_fearful\n",
       "9     female_calm"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actualdf = pd.DataFrame({'actualvalues': actualvalues})\n",
    "actualdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf = actualdf.join(preddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual v/s Predicted emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actualvalues</th>\n",
       "      <th>predictedvalues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>female_angry</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>male_fearful</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>male_happy</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>male_calm</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>male_happy</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>female_angry</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>female_fearful</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>male_happy</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>male_angry</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>female_fearful</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       actualvalues predictedvalues\n",
       "170    female_angry      female_sad\n",
       "171    male_fearful      male_angry\n",
       "172      male_happy    male_fearful\n",
       "173       male_calm       male_calm\n",
       "174      male_happy        male_sad\n",
       "175    female_angry      male_angry\n",
       "176  female_fearful      female_sad\n",
       "177      male_happy      male_happy\n",
       "178      male_angry      male_happy\n",
       "179  female_fearful    female_angry"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf[170:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictedvalues</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actualvalues</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female_angry</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_calm</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_fearful</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_happy</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_sad</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_angry</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_calm</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_fearful</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_happy</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_sad</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predictedvalues\n",
       "actualvalues                   \n",
       "female_angry                 20\n",
       "female_calm                  20\n",
       "female_fearful               16\n",
       "female_happy                 16\n",
       "female_sad                   12\n",
       "male_angry                   20\n",
       "male_calm                    18\n",
       "male_fearful                 23\n",
       "male_happy                   21\n",
       "male_sad                     21"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf.groupby('actualvalues').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actualvalues</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictedvalues</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female_angry</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_calm</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_fearful</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_happy</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_sad</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_angry</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_calm</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_fearful</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_happy</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_sad</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 actualvalues\n",
       "predictedvalues              \n",
       "female_angry               19\n",
       "female_calm                26\n",
       "female_fearful             13\n",
       "female_happy               14\n",
       "female_sad                 18\n",
       "male_angry                 15\n",
       "male_calm                  27\n",
       "male_fearful               19\n",
       "male_happy                 19\n",
       "male_sad                   17"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf.groupby('predictedvalues').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finaldf.to_csv('Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The file 'output10.wav' in the next cell is the file that was recorded live using the code in AudioRecoreder notebook found in the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, sampling_rate = librosa.load('output10.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x23b43824048>"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAFACAYAAADwEe4UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhepusb5osgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23b453ee550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% pylab inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob \n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#livedf= pd.DataFrame(columns=['feature'])\n",
    "X, sample_rate = librosa.load('output10.wav', res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "sample_rate = np.array(sample_rate)\n",
    "mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\n",
    "featurelive = mfccs\n",
    "livedf2 = featurelive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "livedf2= pd.DataFrame(data=livedf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "livedf2 = livedf2.stack().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-18.203564</td>\n",
       "      <td>-21.471836</td>\n",
       "      <td>-22.52221</td>\n",
       "      <td>-21.712259</td>\n",
       "      <td>-22.264288</td>\n",
       "      <td>-20.707904</td>\n",
       "      <td>-21.726444</td>\n",
       "      <td>-21.76865</td>\n",
       "      <td>-24.302736</td>\n",
       "      <td>-22.250634</td>\n",
       "      <td>...</td>\n",
       "      <td>-24.273819</td>\n",
       "      <td>-24.639939</td>\n",
       "      <td>-24.929152</td>\n",
       "      <td>-24.43919</td>\n",
       "      <td>-25.210171</td>\n",
       "      <td>-24.740646</td>\n",
       "      <td>-22.311913</td>\n",
       "      <td>-22.579805</td>\n",
       "      <td>-22.31466</td>\n",
       "      <td>-21.552436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1         2          3          4          5          6    \\\n",
       "           0          0         0          0          0          0          0   \n",
       "0 -18.203564 -21.471836 -22.52221 -21.712259 -22.264288 -20.707904 -21.726444   \n",
       "\n",
       "        7          8          9      ...            206        207        208  \\\n",
       "          0          0          0    ...              0          0          0   \n",
       "0 -21.76865 -24.302736 -22.250634    ...     -24.273819 -24.639939 -24.929152   \n",
       "\n",
       "        209        210        211        212        213       214        215  \n",
       "          0          0          0          0          0         0          0  \n",
       "0 -24.43919 -25.210171 -24.740646 -22.311913 -22.579805 -22.31466 -21.552436  \n",
       "\n",
       "[1 rows x 216 columns]"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "livedf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twodim= np.expand_dims(livedf2, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "livepreds = loaded_model.predict(twodim, \n",
    "                         batch_size=32, \n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.24052530e-22,   0.00000000e+00,   3.62402176e-26,\n",
       "          1.30680162e-36,   4.47264152e-28,   1.00000000e+00,\n",
       "          1.80208343e-30,   2.76873961e-27,   3.62227194e-23,\n",
       "          1.67396652e-11]], dtype=float32)"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "livepreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "livepreds1=livepreds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liveabc = livepreds1.astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male_angry'], dtype=object)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "livepredictions = (lb.inverse_transform((liveabc)))\n",
    "livepredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
